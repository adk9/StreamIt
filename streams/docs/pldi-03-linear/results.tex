\section{Results}
\label{sec:results}

\subsection{Measurement Methodology}
%We chose to measure the strength of our optimizations in terms of 
%floating point instruction reduction. The StreamIt compiler currently
%has two code generation backends. The uniprocessor backend generates sequential C code
%that can be compiled and linked against a supporting library. 
%There is also a backend that generates code for the RAW microprocessor
%\cite{waingold97baring, raw-micro}, which features
%a grid of processors interconnected via various communication facilities. 
%Mapping a StreamIt program on to the RAW architecture is complicated
%by issues of communication, load balancing and partitioning\cite{streamit-asplos}. 
%Therefore, the effects of our linear analysis optimizations can not
%be easily differentiated from differences in placement, routing and fusion that result
%from modifying the program's stream graph (as is the case for linear replacement). 
%Therefore we chose to use the uniprocessor backend for our measurements.

%As always, the appropriate metric to measure performance is not totally clear. 
%Running time is complicated by the multitasking environment offered by 
%modern operating systems. Also, given that the uniprocessor backend for
%the StreamIt compiler is meant for prototyping, the supporting 
%library is anything but optimized. Therefore, measuring running time is
%probably not an appropriate metric.

The fundamental metric of processing performance that we chose to measure is the 
amount of computation. Most of the applications that are and will
be written in StreamIt process floating point data so reducing FLOPs
is a primary goal. Our measurement platform was a Dual Intel 2.2 GHz P4 Xenon processor system 
running Linux. We compiled programs using StreamIt's uniprocessor
and generated executables from the resulting C using {\tt gcc} with {\tt -O2} optimizations.

To measure the number of FLOPs executed at runtime we used the RIO\cite{rio-webpage}
runtime introspection and optimization infrastructure. Using a simple DynamoRIO application 
we instrumented each benchmark program at runtime to count the number of FLOPs.

We define the following three opcode classes for the Intel x86 instruction set:
\begin{itemize}
\item[flops] Any opcode whose name that begins with {\tt f} in the x86 instruction set.
\item[fadds] Any of ({\tt fadd faddp fiaddp fsub fsubp fisubp fsubr fsubrp fisubr}).
\item[fadds] Any of ({\tt fmul fmulp fimulp fdiv fdivp fidivp fdivr fdivrp fidivr}).
\end{itemize}

There are no standard benchmark written for StreamIt, so we had to assemble
a set of programs which perform computations that are likely to be representative
real world applications. We settled on the following set of six.

\begin{enumerate}
\item FilterBank: Implements a multi-rate signal decomposition processing block common in communications and image processing.
\item FIR: A single 100 point rectangularly windowed low pass FIR filter ($\omega_c=\frac{\pi}{3}$).
\item FMRadio: Implements an FM software radio.
\item RateConverter: Downsamples an audio signal by an non integer factor ($\frac{3}{2}$).
\item TargetDetect: Four matched filters performing threshold target detection in parallel.
\item Radar: radar frontend, beamformer and target detection.
\end{enumerate}


\subsection{Frequency Replacement Scaling}

\begin{figure}
\center
\epsfxsize=3.2in
\epsfbox{images/frequency-win-theory.eps}
\caption{Plot showing the theoretical multiplication reduction factor as a function of the size of the FIR ($M$) and the number of outputs produced ($N$). The dark regions denote an increase in the required number of multiplications.}
\label{fig:frequency-win-theory}
\vspace{-12pt}
\end{figure}

Direct convolution requires $O(MN)$ multiplies to generate $N$ output values. 
Implementing convolution using an FFT algorithm to calculate the DFT requires 
$O((M+N)lg(M+N))$ multiplications for both the conversion to and from the frequency domain
and $O(M+N)$ multiplications in the frequency domain and produces $M+N$ outputs.
We define the multiplication savings factor as the number of multiplies per output
using direct convolution over the number of multiplies per output using the 
frequency transformation. The reduction factor 
$f(M,N)=\frac{O(M(M+N))}{O(M+N)lg(M+N))+(M+N))}$. 
Figure~\ref{fig:frequency-win-theory} shows a plot of this function in normalized 
$M$, $N$ space. 

We also performed experiments to measure the actual multiplication reduction
as a function of $M$ (filter size) and output size ($N$).
Figure~\ref{fig:frequency-win-empirical} shows the empirically measured
multiplication savings factor per output.

\begin{figure}
\center
\epsfxsize=3.2in
\epsfbox{images/frequency-win-empirical.eps}
\caption{Plot showing the empirically measured multiplication reduction factor as a function of the size of the FIR ($M$) and the number of outputs produced ($N$). The dark regions denote an increase in the required number of multiplications.}
\label{fig:frequency-win-empirical}
\vspace{-12pt}
\end{figure}

Our experimental results have the same shape, however we don't get the same
speedup because of constant factors hidden by the assymptotic notation. For instance,
elementwise multiplication of the complex values in $H[k]$ and $X[k]$ require
four floating point operations for a straightforward implementation.



\subsection{Simplificaton from Combination}
Linear analysis is valuable because it provides a precise relationship between input and output   
that are impossible for normal compilers to discern from imperative programs. By 
combining the actions of StreamIt structures we can do optimizations that are 
not feasible for a standard compiler.

Combining the action of {\tt streams} in a {\tt pipeline} is an equivalent optimization 
to algebraic simplification across sequential loops in an imperative program.
Once an overall linear form for the {\tt pipeline} has been determined, our compiler 
can simply replaces the original {\tt pipeline} with a {\tt filter} that computes the 
combined linear form. Since the compiler can perform this transformation automatically,
the the programmer can now express the computation in the most convenient manner. 

For instance, although a bandpass filter can be implemented with a 
low pass filter followed by a high pass filter, real implementations determine the coefficients 
of a single filter that computes the same values. While a simple bandpass filter is easy to combine
manually, in an actual design several different filters might be designed by 
several different people which would be much harder to integrate into a single design.

A common operation in discrete time signal processing is downsampling
Downsampling is most often implemented as a low pass filter followed by a 
an $M$ compressor which passes every $M$th input sample to the output.
For every output of the downsampler, $M-1$ outputs from the low pass filter
are ignored, so only every $Mth$ sample needs be computed.
While this optimization is almost always done in actual system implementations because it reduces 
the computational requirements by a factor of $\frac{M-1}{M}$, in the system specifications
very often both the filter and the compressor will appear. The inefficient
implementation is represented because it is conceptually easier to understand. 
Our linear analysis covers this case, and thus our compiler can implement the transformation
automatically.

Another example is a multi band equalizer (such as is found in the FMRadio benchmark). 
The purpose of such an equalizer is to filter $N$ different frequency bands 
separately (like the graphic equalizer found on stereos). The intuitive system specification 
of an equalizer consists of $N$ bandpass filters operating in parallel whose output 
is combined to produce the equalized signal. However, if the filters do not vary with time, 
a single filter which performs the equivalent computation can be substituted instead. 
However, designing this single combined filter is difficult, and any subsequent changes to 
any one of the sub filters will necessitate a redesign of the overall filter. 
Out compiler can automatically derive the equivalent filter, and any subsequent design changes
will only necessitate a recompile rather than a manual redesign.


\subsection{Overall optimization performance}

\begin{figure}
\center
\epsfxsize=3.2in
\epsfbox{images/overlap-add-savings.eps}
\vspace{-12pt}
\caption{Multiplication savings from using the overlap and add method compared to overlap and discard method.}
\label{fig:overlap-add-savings}
\vspace{-12pt}
\end{figure}

Figure~\ref{fig:overlap-add-savings} shows the required computation differences for
the overlap-add method and the overlap-discard method. 
In all cases, the overlap-add method performs better than the overlap-discard method.
The performance difference declines as the number of outputs($N$) increases because
the ratio of the amount of data discarded by the overlap-discard method to the 
amount of data produced grows smaller. 

\begin{figure}
\center
\epsfxsize=3.2in
\epsfbox{images/linear-freq-both.eps}
\vspace{-6pt}
\caption{Multiplication reduction factors for each of the benchmarks with linear replacement, frequency replacement, and both.}
\label{fig:linear-freq-both}
\vspace{-12pt}
\end{figure}

To determine the effects of our linear replacement and frequency replacement 
optimizations, we compiled each benchmark with the linear 
replacement, frequency replacement (overlap-add),
and with linear replacement followed by frequency replacement. 
Figure~\ref{fig:linear-freq-both} shows the multiplication reduction factor 
achieved for each set of optimizations.

For the FIR filter, all of the multiplication reduction comes from the frequency
replacement optimization because the entire application is comprised of a single
{\tt filter} calculating a convolution sum and there is nothing to combine.
The TargetDetect program experiences a small decrease in multiplications 
for linear replacement, and a larger boost from frequency replacement alone.
The main work of the target detector system occurs in the four parallel FIR filters, whose
results can not be combined.

However, linear replacement reduces the number of multiplies 
requried for FilterBank by a factor of three because the parallel analysis and 
synthesis pipelines can be combined. Unfortunately, after
linear replacement, the frequency translation doesn't reduce computation 
because our current implementation only works for one column linear filters.

The RateConverter also benefits greatly from the frequency transformation
due to the fact that it has contains a large FIR filter. Our implementation
currently doesn't support the full range of pipeline combination rules so
we miss potential savings from linear combination.

Similairly, FMRadio contains a multiband equalizer whose gain stages are collapsed
into a single filter during linear combination.

The number of multiplies in the Radar program actually increases because our 
compiler does not take into account the fact that combined linear structures 
might actually increase computation. %this is bs, obviously...


\begin{table*}[t]
\begin{tabular}{|l|c|c||c|c||c|c||c|} 
\hline
Benchmark & Filters & Linear  & Pipelines & Linear    & SplitJoins & Linear     & Average  \\
          &         & Filters &           & Pipelines &            & SplitJoins & Vector Size\\
\hline
FIRProgram & 3 & 1 & 1 & 0 & 0 & 0 & 100 \\
\hline
TargetDetect & 10 & 4 & 1 & 0 & 2 & 1 & 100 \\
\hline
FilterBank & 27 & 25 & 17 & 9 & 4 & 3 & 51 \\
\hline
SamplingRateConverter & 5 & 3 & 2 & 0 & 0 & 0 & 335 \\
\hline
FMRadio & 25 & 22 & 3 & 1 & 2 & 2 & 33 \\
\hline
BeamFormer & 65 & 60 & 33 & 14 & 2 & 0 & 46 \\
\hline
\end{tabular}
\caption{Statistics for benchmarks before and after transformations.}
\label{fig:benchmark-stastics}
\end{table*}
