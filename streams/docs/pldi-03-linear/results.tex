\section{Results}
\label{sec:results}

Our compiler currently has two linear analysis optimizations. The first,
{\it linear replacement}, replaces the largest linear heirarchal {\tt streams} 
possible with filters that directly compute the calculation specified by 
the associated linear node. The second optimization, called 
{\it frequency replacement}, replaces all {\tt streams} which implement 
a convolution using the frequency transformation described in 
\ref{sec:freq}. Below we describe experiements and results which measure the
performance of these two optimizations.



\subsection{Measurement Methodology}
%We chose to measure the strength of our optimizations in terms of 
%floating point instruction reduction. The StreamIt compiler currently
%has two code generation backends. The uniprocessor backend generates sequential C code
%that can be compiled and linked against a supporting library. 
%There is also a backend that generates code for the RAW microprocessor
%\cite{waingold97baring, raw-micro}, which features
%a grid of processors interconnected via various communication facilities. 
%Mapping a StreamIt program on to the RAW architecture is complicated
%by issues of communication, load balancing and partitioning\cite{streamit-asplos}. 
%Therefore, the effects of our linear analysis optimizations can not
%be easily differentiated from differences in placement, routing and fusion that result
%from modifying the program's stream graph (as is the case for linear replacement). 
%Therefore we chose to use the uniprocessor backend for our measurements.

%As always, the appropriate metric to measure performance is not totally clear. 
%Running time is complicated by the multitasking environment offered by 
%modern operating systems. Also, given that the uniprocessor backend for
%the StreamIt compiler is meant for prototyping, the supporting 
%library is anything but optimized. Therefore, measuring running time is
%probably not an appropriate metric.


Both linear replacement and frequency replacement increase performance primarily by 
decreasing the number of floating point multiplications required per output.
Our measurement platform was a Dual Intel 2.2 GHz P4 Xenon processor system 
with 2GB of memory running Linux. We compiled programs using StreamIt's uniprocessor backend
and generated executables from the resulting C using {\tt gcc} with {\tt -O2} optimization.

To measure the number of FLOPs executed at runtime we used the DynamoRIO\cite{rio-webpage, dynamo99}
infrastructure. Using a simple DynamoRIO application 
we instrumented each benchmark program at runtime to count the number of FLOPs.

We define the following three opcode classes for the Intel x86 instruction set:
\begin{itemize}
\item[flops] Any opcode whose name that begins with {\tt f} in the x86 instruction set.
\vspace{-6pt}
\item[fadds] Any of ({\tt fadd faddp fiaddp fsub fsubp fisubp fsubr fsubrp fisubr}).
\vspace{-6pt}
\item[fadds] Any of ({\tt fmul fmulp fimulp fdiv fdivp fidivp fdivr fdivrp fidivr}).
\vspace{-6pt}
\end{itemize}

There are no standard benchmarks written for StreamIt, so we use
a set of representative programs which perform computations that 
are found in actual applications: 

\begin{enumerate}
\item FilterBank: Implements a multi-rate signal decomposition processing block common in communications and image processing.
\vspace{-6pt}

\item FIR: A single 100 point rectangularly windowed low pass FIR filter ($\omega_c=\frac{\pi}{3}$).
\vspace{-6pt}

\item FMRadio: An FM software radio with equalizer.
\vspace{-6pt}

\item RateConverter: A downsampler for an audio signal that converts
the rate by a non-integral factor ($\frac{3}{2}$).
\vspace{-6pt}

\item TargetDetect: Four matched filters performing threshold target
detection in parallel.
\vspace{-6pt}

\item Radar: A radar frontend with beamformer and target detection.
\end{enumerate}

\subsection{Frequency Replacement Scaling}

\begin{figure*}[t]
\center
\begin{minipage}{3.4 in}
\center
\epsfxsize=2.2in
\epsfbox{images/frequency-win-theory.eps} \\
{\bf (a): Theoretical}
\end{minipage} 
\begin{minipage}{3.4 in}
\center
\epsfxsize=2.2in
\epsfbox{images/frequency-win-empirical.eps} \\
{\bf (b): Empirical}
\end{minipage}
\caption{Plots showing the theoretical and emphirical multiplication reduction factor as a function of the size of the FIR ($M$) and the number of outputs produced ($N$). The dark regions denote an increase in the required number of multiplications.}
\label{fig:frequency-win}
\vspace{-12pt}
\end{figure*}

Direct convolution requires $O(MN)$ multiplies to generate $N$ output
values. The FFT requires $O(N+2(M-1))lg(N+2(M-1))$ multiplications for both the
conversion to and from the frequency domain, and multiplying two $N+2(M-1)$
vectors in the frequency domain requires $O(N+2(M-1))$. After the frequency transformation,
$N+M-1$ outputs are produced every execution. We define the multiplication reduction factor
as the number of multiplies required per output using convolution divided by the 
the number of multiplies per output using the frequency transformation. 

Figure~\ref{fig:frequency-win}~(a) shows a plot of the theoretical multiplication savings
in $M$, $N$ space and Figure~\ref{fig:frequency-win}~(b) shows our empirically 
measured savings. The roughness of both the theory and the data is due to the fact that
our FFT algorithm requires $N+2(M-1)$ to be a power of two, and the compiler
automatically adjusts $N$ upwards to satisfy this requirement. The theoretical 
reduction numbers account for the fact that our implementation requires four 
floating point muliplication operations to perform a complex valued multiply 
in the frequency domain.

\subsection{Simplification from Combination}
Linear analysis is valuable because it provides a precise relationship
between input and output that is nearly impossible to extract from
general-purpose imperative programs. By combining the actions of
StreamIt structures by combining their linear nodes we can do 
optimizations that are impossible for a standard compiler.

Combining the action of {\tt streams} in a {\tt pipeline} is equivalent 
to algebraic simplification between the bodies of sequential loops in 
an imperative program. Once an overall linear node for the
{\tt pipeline} has been determined, our compiler can perform this highly non-trivial 
transformation automatically, leaving the the programmer to express 
the computation in the most convenient manner.

For example, although a bandpass filter can be implemented with a low
pass filter followed by a high pass filter, actual implementations
tend to determine the coefficients of a single combined filter that 
performs the same computation. While a simple bandpass filter is 
easy to combine manually, in an actual design several different 
filters might be designed and implemented by several different people 
which would be much harder to integrate into a single design.

A common operation in discrete time signal processing is downsampling
to reduce the computational requirements of a system.
Downsampling is most often implemented as a low pass filter followed
by an $M$ compressor which passes every $M$th input sample to the
output.  Therefore, $M$ outputs from the
low pass filter are calculated for every overall output, when in actual fact
only every $M$th sample needs be computed.  Almost all actual systems 
do not calculate the $M-1$ unused outputs which decreases computational
requirements by a factor of $\frac{M-1}{M}$. However, most often 
the system's actual specification will include both the filter and the compressor. 
This inefficient implementation appears because it is
conceptually easier to understand. Our linear analysis and transformations
can describe the computation well enough enough so that our compiler can
derive the efficient transformation automatically.

Another example of how combining linear nodes can help overall performance
is a multi-band equalizer (such as is found in the
FMRadio benchmark).  The purpose of such an equalizer is to filter $N$
different frequency bands separately, exactly like the graphic equalizer found
on stereos. The intuitive system specification of an equalizer
consists of $N$ bandpass filters operating in parallel whose output is
combined to produce the equalized signal. However, if the filters do
not vary with time, a single filter which performs the equivalent
computation can be substituted instead. Designing this
single overall filter is difficult, and any subsequent changes to any
one of the sub filters will necessitate a total redesign of the
filter. Using the linear analysis and combination presented,
our compiler can automatically derive the equivalent filter from the
intuitive specification. Therefore any subsequent design changes will 
necessitate only a recompile rather than a manual redesign.


\subsection{Overall optimization performance}

%\begin{figure}
%\center
%\epsfxsize=3.2in
%\epsfbox{images/overlap-add-savings.eps}
%\vspace{-12pt}
%\caption{Multiplication savings from using the overlap and add method compared to overlap and discard method.}
%\label{fig:overlap-add-savings}
%\vspace{-12pt}
%\end{figure}

%Figure~\ref{fig:overlap-add-savings} shows the required computation differences for
%the overlap-add method and the overlap-discard method. 
%In all cases, the overlap-add method performs better than the overlap-discard method.
%The performance difference declines as the number of outputs($N$) increases because
%the ratio of the amount of data discarded by the overlap-discard method to the 
%amount of data produced grows smaller. 

\begin{figure}
\center
\epsfxsize=3.2in
\epsfbox{images/linear-freq-both.eps}
\vspace{-6pt}
\caption{Multiplication reduction factors for each of the benchmarks with linear replacement, frequency replacement, and both.}
\label{fig:linear-freq-both}
\vspace{-12pt}
\end{figure}

To determine the effects of our linear replacement and frequency replacement 
optimizations, we compiled each benchmark program with the linear 
replacement, wtih frequency replacement (overlap-add),
and with linear replacement followed by frequency replacement. 
Figure~\ref{fig:linear-freq-both} shows the multiplication reduction factor 
achieved for each set of optimizations.

For the FIR filter, all of the multiplication reduction comes from the frequency
replacement optimization because the entire application is comprised of a single
{\tt filter} calculating a convolution sum so there is nothing to combine.
The TargetDetect program experiences a small decrease in multiplications 
for linear replacement, and a larger boost from frequency replacement alone.
The main work of the target detector system occurs in the four parallel FIR filters, whose
results can not be combined.

The linear replacement optimization reduces the number of multiplies 
required for FilterBank by a factor of three because the parallel analysis and 
synthesis pipelines can be combined. Unfortunately, after
linear replacement, the frequency translation doesn't reduce computation 
because our current implementation only works for one column linear filters.

The RateConverter also benefits greatly from the frequency
transformation due to the fact that it contains a large FIR
filter. Our implementation currently doesn't support the full range of
pipeline combination rules so we miss potential savings from linear
combination. Similairly, FMRadio contains a multiband equalizer whose gain stages are collapsed
into a single filter during linear combination.

The number of multiplies in the Radar program actually increases because our 
compiler does not take into account the fact that combined linear structures 
might actually increase computation. %this is bs, obviously...

\begin{figure}
\center
\epsfxsize=3.2in
\epsfbox{images/execution-speedup.eps}
\vspace{-6pt}
\caption{Execution time reduction for each of the benchmarks with both linear replacement and frequency replacement.}
\label{fig:execution-speedup}
\vspace{-12pt}
\end{figure}

Figure~\ref{fig:execution-speedup} shows the execution time decrease due to the back to back
application of the linear replacment and frequency replacement options.


\begin{table*}[t]
\centering
\small
\begin{tabular}{|l|c|c|c||c||c|c|c|} 
\hline
          & \multicolumn{3}{|c||}{Originally}  &             & \multicolumn{3}{|c|}{After Linear Replacement} \\
\hline
Benchmark & Filters & Pipelines & SplitJoins & Average     & Filters      & Pipelines         & SplitJoins \\
          & (linear)& (linear)  & (linear)   & vector size &              &                   &            \\
\hline
FIRProgram & 3 (1) & 1(0) & 0 (0) & 100 & 3 & 1 & 0 \\
\hline
TargetDetect & 10 (4) & 1 (0) & 2 (1) & 100 & 7 & 1 & 1 \\
\hline
FilterBank & 27 (25) & 17 (9) & 4 (3) & 51 & 15 & 8 & 1 \\
\hline
SampleRate & 5 (3) & 2 (0) & 0 (0) & 335 & 5 & 2 & 0 \\
\hline
FMRadio & 25 (22) & 3 (1) & 2 (2) & 33 & 5 & 1 & 0 \\
\hline
BeamFormer & 65 (60) & 33 (14) & 2 (0) & 46 & 37 & 17 & 2\\
\hline
\end{tabular}
\caption{Statistics for benchmarks before and after transformations.}
\label{fig:benchmark-stastics}
\end{table*}
