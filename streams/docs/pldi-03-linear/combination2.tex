%  could replace ``filter'' by ``stream'' in the next few sentences, but 
% I think it reads better as it is, actually
\section{Combining Linear Filters}
A primary benefit of linear filter analysis is that neighboring
filters can be collapsed into a single matrix representation if both
of the filters are linear.  This transformation automatically
eliminates redundant computations in linear sections of the stream
graph, thereby allowing the programmer to write simple, modular
filters and leaving the combination to the compiler.  In this section,
we first describe a {\it linear expansion} operation that serves as a
building block for the combination techniques.  We then give rules for
collapsing {\tt pipeline}s and {\tt splitjoin}s into linear nodes; we
do not yet deal with {\tt feedbackloop}s as they require the notion of
``linear state'' (see Future Work).

\subsection{Linear Expansion}

In StreamIt, the input and output rate of each {\tt filter} in the
stream graph is known at compile time.  The StreamIt compiler
leverages this information to compute a static schedule--that is, an
ordering of the node executions such that each filter will have enough
data available to atomically execute its {\tt work} function, and no
buffer in the stream graph will grow without bound in the steady
state.  A general method for scheduling StreamIt programs is given by
Karczmarek~\cite{karczma-thesis}.

A fundamental aspect of the steady-state schedule is that neighboring
nodes might need to be fired at different frequencies.  For example,
if there are two {\tt filters} $F_1$ and $F_2$ in a {\tt pipeline} and
$F_1$ produces $2$ elements during its {\tt work} function but $F_2$
consumes $4$ elements, then it is necessary to execute $F_1$ twice for
every execution of $F_2$.

%% \begin{figure}
%% \center
%% \epsfxsize=3.0in
%% \epsfbox{images/expanding-a-filter.eps}
%% \caption{Expanding {\tt stream} $S$ by a factor $f$}
%% \label{fig:expanding-a-filter}
%% \vspace{-12pt}
%% \end{figure}

Consequently, when we combine hierarchical structures into a linear
node, we often need to {\it expand} a matrix representation to
represent multiple executions of the corresponding stream.  This
expansion can be done as follows.

% NOT TRUE ANYMORE
%% If we expand a linear node by a factor of $k$, then one execution of
%% the new node will be exactly equivalent to $k$ executions of the
%% original.

\begin{definition} (Linear expansion)
% DOUBLE-CHECK whether we're assuming 0-indexed or 1-indexed matrices
Given a linear node $\lambda = \{A, b, e, o, u\}$, the expansion of
$\lambda$ to a rate of $(e', o', u')$ is given by $\mbox{\bf
expand}(\lambda, e', o', u') = \{A', b', e', o', u'\}$, where
$\mathbf{A'}$ is a $u' \times e'$ matrix and $\mathbf{b'}$ is a
$u'$-element row vector:
\vspace{-6pt} \\
\begin{equation} \nonumber
\begin{array}{rcl}
\mathbf{A'}[i,j] & = & \mt{even}(i,j) ~\mt{if}~ (j \ge u'~\mt{mod}~u) \\
~ & = & \mt{even}(i+o, j+u'~\mt{mod}~u) ~\mt{otherwise} \\ ~ \vspace{-4pt} \\
%
\mt{even}(k)& = & \sum_{m=0}^{\lfloor u' / u \rfloor}
\mt{shift}(u'- u -m*u, \\
~ & ~ & ~ \hspace{0.68in}~ e'-e-m*o) \\ ~ \vspace{-4pt} \\
%
\multicolumn{3}{l}{\mt{shift}(r,c) ~\mt{is a}~ u' \times e' ~\mt{matrix}:} \\ ~ \vspace{-8pt} \\
\mt{shift}(r,c)[i,j] & = & A[i-r,j-c] \\
~ & ~ & ~~~~~\mt{if}~ i-r \in [0,e] \wedge j-c \in [0, u] \\
~ & = & 0 \mt{~otherwise} \\ \vspace{-4pt} \\
%
\mathbf{b'}[j] & = & b[u-1-(u'-j-1)~\mt{mod}~u]
%%
%% \raisebox{-14pt}{\parbox{3in}{is created by starting with a zero
%% matrix with $e'$ rows and $u'$ columns.  $A$ is then copied $k$ times
%% along the diagonal. Starting at the top left, each copy of $A$ is
%% offset from the previous copy by $u$ columns and $o$ rows.}} \\ ~ \vspace{-8pt} \\
%% $b'$ & is an row vector containing $k$ adjacent copies of $b$.
\end{array}
\end{equation}
\end{definition}

Despite the bulky notation above, the intuition behind linear
expansion is straightforward (see
Figure~\ref{fig:expanding-a-matrix}.)  Linear expansion aims to scale
the push, pop, and peek rates of a linear node while preserving the
functional relationship between the values pushed and the values
peeked on a given execution.  To do this, we construct a new matrix
$A'$ that contains copies of $A$ along the diagonal.  To account for
items that are popped between invocations, each copy of $A$ is offset
by $o$ from the previous copy.  The complexity of the definition is
due to the end cases.  If the new push rate $u'$ is not a multiple of
the old push rate $u$, then the last copy of $A$ includes only some of
its columns.  Similarly, if the new peek rate $e'$ exceeds that which
is needed by the diagonal of $A$s, then $A'$ needs to be padded with
zero's at the top (since it peeks at some values without using them in
the computation.)

Note that a sequence of executions of an expanded node $\lambda'$
might not be equivalent to any sequence of executions of the original
node $\lambda$, because expansion resets the push and pop rates and
can thereby modify the ratio between them.  However, if $u' = k * u$
and $o' = k * o$ for some integer $k$, then $\lambda'$ is completely
interchangebale with $\lambda$.  In the combination rules that follow,
we utilize linear expansion both in contexts that do and do not
satisfy this condition.

\begin{figure}
\center
\epsfxsize=3.0in
\epsfbox{images/expanding-a-matrix.eps}
\caption{Expanding a linear node to have I/O rates $(e', o', u')$.  }
\label{fig:expanding-a-matrix}
\vspace{-12pt}
\end{figure}

\subsection{Collapsing Linear Pipelines}

\begin{figure}
\center
\epsfxsize=3.0in
\epsfbox{images/pipeline-combination.eps}
\caption{A {\tt pipeline} of two linear forms $(A,b)$ and $(C,d)$ (above) and the same {\tt pipeline} with rate matched forms (below).}
\label{fig:combining-pipeline}
\vspace{-12pt}
\end{figure}

The {\tt pipeline} construct is used to compose streams in sequence,
with the output of stream $i$ connected to the input of stream $i+1$.
The following transformation describes how to collapse two linear
nodes in a {\tt pipeline}; it can be applied repeatedly to collapse
any number of neighboring linear nodes.

\begin{combination} (Linear pipeline)
Given two linear nodes $\lambda_1$ and $\lambda_2$ where the output of
$\lambda_1$ is connected to the input of $\lambda_2$ in a pipeline
construct, the combination $\mbox{\bf pipe}(\lambda_1, \lambda_2)$ $=$
$\mathbf{ \{A', b', e', o', u'\}}$ represents an equivalent node that
can replace the original two.  Its components are as follows: \\
\begin{equation} \nonumber
\begin{array}{rcl}
L & = & \mt{lcm}(u_1, o_2) \\
\lambda_1^e & = & \mt{expand}(\lambda_1, \frac{o_1 * L}{u_1} + e_1 - o_1, 
\frac{o_1 * L}{u_1}, L+e_2-o_2) \\
\lambda_2^e & = & \mt{expand}(\lambda_2, L+e_2-o_2, L, \frac{u_2 * L}{o_2}) \\
\mathbf{A'} & = & A_1^e A_2^e \\
\mathbf{b'} & = & b_1^e A_2^e + b_2^e \\
\mathbf{e'} & = & e_1^e \\
\mathbf{o'} & = & o_1^e \\
\mathbf{u'} & = & u_2^e
\end{array}
\end{equation}
\end{combination}

It is straightforward to derive the equations above.  Let $x_i$ and
$y_i$ be the input and output channels, respectively, for $\lambda_i$.
Then we have by definition that $y_1 = x_1 A_1 + b_1$ and $y_2 = x_2
A_2 + b_2$.  But since $\lambda_1$ is connected to $\lambda_2$, we
have that $x_2 = y_1$ and thus $y_2 = y_1 * A_2 + b_2$.  Substituting
the value of $y_1$ from our first equation gives $y_2 = x_1 A_1 * A_2
+ b_1 A_2 + b_2$.  Thus, the intuition is that the two-filter sequence
can be represented by matrices $A' = A_1 * A_2$ and $b' = b_1 A_2 +
b_2$, with peek and pop rates borrowed from $\lambda_1$ and the push
rate borrowed from $\lambda_2$.

However, there are two implicit assumptions in the above analysis
which complicate the equations for the general case.  First, the
dimensions of $A_1$ and $A_2$ must match for the matrix multiplication
to be well-defined.  If $u_1 \ne e_2$, this will require constructing
expanded nodes $\lambda_1^e$ and $\lambda_2^e$ in which the push and
peek rates match (and thus $A_1^e$ and $A_2^e$ can be multiplied.)

The second complication is with regards to peeking.  If the downstream
node $\lambda_2$ peeks at items which it does not consume ({\it i.e.},
if $e_2 > o_2$), then there needs to be a buffer to hold items that
are read during multiple invocations of $\lambda_2$.  However, in our
current formulation, a linear node has no concept of internal state,
such that this buffer cannot be incorporated into the collapsed
representation.  To deal with this issue, we adjust the expanded form
of $\lambda_1$ to recalculate items that $\lambda_2$ uses more than
once, thereby trading computation for storage space.  This adjustment
is evident in the push and pop rates chosen for $\lambda_1^e$: though
$\lambda_1$ pushes $\frac{u_1}{o_1}$ for every item that it pops, for
$\lambda_1^e$ this ratio is $\frac{u_1^e}{o_1^e}$ $=$
$\frac{L+e_2-o_2}{o_1 * L / u_1}$ $=$
$(\frac{u_1}{o_1})(\frac{L+e_2-o_2}{L})$, which is larger due to the
$e_2-o_2$ items that are regenerated for consumption by $\lambda_2$.

Note that although $\lambda_1^e$ performs duplicate computations in
the case where $\lambda_2$ is peeking, this redundancy disappears in
the collapsed node $\mt{pipe}(\lambda_1, \lambda_2)$ because
overlapping operations are combined in the matrix multiplication.  One
way to see this is that the dimensions of the matrix $A'$ are
independent of $e_2$, the peek rate of $\lambda_2$.  That is, $A'$ has
$e'$ $=$ $e_1^e$ $=$ $\frac{o_1*\mt{lcm}(u_1, o_2)}{u_1}+e_1-o_1$ rows
and $u'$ $=$ $u_2^e$ $=$ $\frac{u_2 * \mt{lcm}(u_1, o_2)}{o_2}$
columns; neither of these expressions depends on $e_2$.

However, it is the case that a collapsed linear node can be less
efficient than the original pipeline sequence.  The worst case is when
$A_1^e$ is a column vector and $A_2^e$ is a row vector, which requires
$O(n)$ operations originally but $O(n^2)$ operations if combined
(assuming the vectors are of length $n$).  In general, the compiler
can identify performance-degrading combinations when the number of
non-zero elements in $A'$ exceeds the sum of non-zero elements in
$A_1^e$ and $A_2^e$.
% ARE WE SURE THAT LAST SENTENCE IS TRUE?  I think it's more complicated 
% when the pop/peek rate is varied, etc.

\subsection{Collapsing Linear SplitJoins}
{\tt splitjoin}s allow a StreamIt programmer to express explicitly
parallel computation.  Data elements that arrive at the {\tt
splitjoin} are directed to the parallel sub {\tt streams} in one of
two ways.  The simpler of the two is to send copy of each data element
to the each of the sub {\tt streams}.  The second of the two is
specified by $N$ weights, $v_k$ for $k\in[0..N-1]$, where $N$ is the
number of sub {\tt streams} in the {\tt splitjoin}. The first $v_0$
data elements are sent to the leftmost {\tt Stream}. The next $v_1$
elements are sent to the next {\tt stream} and so on. When all
$\sum_{k=0}^{N-1} v_k$ elements have been seen, the splitter starts
the same pattern over again.

The data from the parallel {\tt streams} are combined back into a single stream by means of
a joiner specified by weights $w_k$ for $k\in[0..N-1]$. First, $w_0$ items from the output tape of the 
leftmost {\tt stream} are placed onto the overall output tape, then 
$w_1$ elements are taken from the second leftmost {\tt stream} and so on. 
Again, the process repeats itself after one complete set of $\sum_{k=0}^{N-1} w_k$ 
elements has been pushed.

If we know that all subcomponents of a {\tt splitjoin} 
have linear representations, then we can describe the action of the entire {\tt splitjoin} 
with a single linear representation.
{\tt splitjoin}s containing all linear components are not as infrequent as they might seem -- 
most {\tt splitjoin} sub {\tt streams} often perform very similar computations, so when one 
is linear there is a high probability that all linear.

The joiner of a {\tt splitjoin} specifies a specific order of the output data 
that is produced by the parallel {\tt stream} blocks. Because each column 
of a linear representation's matrix represents the formula for calculating a single output value, 
the overall linear representation of a combined {\tt splitjoin} will be exactly 
the same columns of the parallel {\tt streams} in an ordering that depends on the joiner weights $w_i$.

Below we will describe the combination rules for a {\tt splitjoin}
with a duplicate splitter. Then we will describe how we can transform 
a roundrobin splitter {\tt splitjoin} into a duplicate splitter {\tt splitjoin}. 


\subsubsection{Duplicate Splitter}

\begin{figure}
\center
\epsfxsize=3.0in
\epsfbox{images/splitjoin-duplicate-ratematch.eps}
\caption{Expanding sub {\tt streams} to match their output rates in a linear {\tt SplitJoin}.}
\label{fig:splitjoin-duplicate-ratematch}
\end{figure}

In the following discussion, we focus on the matrix $A$ of a linear representation.
The operations for the constant vector $a$ occur in a very similar way.

Each row in the matrix $A$ of a linear form corresponds to the
weight associated with a particular input item. For a duplicate splitter,
the same rows in the sub matrices correspond to the same
input so long as the number of rows in the sub matrices are the same.

We wish to produce an overall representation by determining the correct
ordering the columns of the sub {\tt stream}' linear representations. We
need the number of rows to be the same, the total number of columns 
from all the sub matrices to equal some multiple of $\sum_{i=0}^{N-1}w_i$
and we need all of the sub linear forms to have the same $o$. 

If the joiner is ``run'' an integer number of times it needs to consume in aggregate
the exact number of items produced by an integer number of firings of
each sub {\tt stream}. To guarantee that there are never any ``spare'' columns,
we expand each of the sub {\tt streams} $F_i$ by a factor $f_i$.

Let $x=lcm(\forall k, lcm(u_{F_k},w_k))$, which is the $lcm$ of the $lcm$s of 
the push rates and the joiner weights. $x$ is the total number of elements that 
produced by a {\tt splitjoin} whose output is the same as the original but each 
of its sub {\tt streams} can be executed an integer number of times so that 
the joiner executes an integer number of times.

Therefore the appropriate expansion factors are  $f_i=\frac{x}{u_{F_i}}$ 
$F''_i = expand(F_i,f_i)$ will produce $f_i*u_{F_i}$ items, and it will take 
exactly $f_{w_i}=\frac{x}{w_i}$ firings of the 
joiner (each of which consumes $w_i$ elements), to consume all $f_iu_i$ items produced by $F''_i$.
It is important to note that we require that the {\tt splitjoin} does not need
infinite buffer space to schedule see \cite{karczma-thesis} for an explanation of schedules. 
For valid {\tt splitjoin}s $f_w=f_{w_i}$ is constant for all $i$. We also require that $o=o_{F''_i}$
constant for all $i$. Figure~\ref{fig:splitjoin-duplicate-ratematch} depicts the 
combination process graphically.

\begin{figure}
\center
\epsfxsize=3.0in
\epsfbox{images/splitjoin-duplicate-matrix.eps}
\caption{Matrix resulting from combining a {\tt splitjoin} of rate matched sub {\tt streams}.}
\label{fig:splitjoin-duplicate-matrix}
\end{figure}

After the rates are matched as described above, we are guaranteed to have the same number of rows
in each of the expanded sub representations. To construct the overall matrix, $A'''$, 
all that remains is to order the columns of the expanded representations appropriately. 
First we create $A'''$ which has $e_{F''_i}$ rows (which is the same for all $i$) and has 
$f_w(\sum_{i=0}^{N-1}w_i)$ columns. 
%We know that the first $w_0$ output elements of the overall {\tt splitjoin} will be the first $w_0$ output elements from $F''_1$. 
We place the $w_0$ rightmost columns of $A''_0$ as the $w_0$ rightmost 
columns of $A'''$. We then place the $w_1$ rightmost columns of $A''_1$ into the next $w_1$ columns 
of $A'''$, and so forth. When we have placed $\sum_{i=0}^{N-1}w_{i}$ (the number of outputs resulting from 
completely executing the joiner once), we start the process again from the beginning (taking the remaining
rightmost $w_0$ columns from $A''_0$, etc) until all of the columns from the expanded representations have
been fitted into $A'''$. Figure~\ref{fig:splitjoin-duplicate-matrix} graphically depicts the process of 
creating the overall $A'''$ from the smaller $A''_i$s.



\subsubsection{Roundrobin Splitter}

In the case of roundrobin splitters, data is directed to the various sub {\tt streams} 
according to weights $v_i$: the first $v_0$ input elements are directed to $F_0$, the next
$v_1$ elements are directed to $F_1$, etc. If the sub {\tt streams} are
all linear, we can determine an overall linear representation given the same
assumptions of finite buffer requirements, and uniform $o_i$.
Our strategy is to transform the splitter from roundrobin to duplicate by
modifying the sub {\tt stream}s' linear representations. 

Because the splitter is a roundrobin and not a duplicate, the same rows of the sub 
{\tt stream}s' representation matrices do not represent the same input elements 
even when the sub {\tt stream}s' representations are the same size. 
The same rows don't represent the same data because each {\tt stream} 
doesn't see any of the data directed to any other {\tt stream}. Therefore simply 
arranging columns as described above will not produce a correct overall linear representation. 

\begin{figure}
\center
\epsfxsize=3.0in
\epsfbox{images/splitjoin-roundrobin-matrix.eps}
\caption{Corresponding matrix for splitter conversion from roundrobin to duplicate.}
\label{fig:splitjoin-roundrobin-matrix}
\end{figure}

To perform the transformation from roundrobin to duplicate, the same rows
in each sub {\tt Stream}'s matrix must represent the same data items. Changing from splitjoin to 
duplicate implies that each sub {\tt streams} will be fed all of the input data. All data
that was nor originally bound for other {\tt stream}s needs to be ignored, which requires
changing the original {\tt stream}s $F_i$ to $F'_i$. In our linear
framework, we represent the independence of an output from an input as a zero in the appropriate
element of the linear representation matrix. Therefore, we transform $F_i$ to $F'_i$ by inserting
rows of zeros into linear representation of the sub {\tt streams} at positions such that
the data bound for another sub {\tt stream} is ignored. Each of the new matrices $F'_i$ 
has exactly $\sum_{i=0}^{N-1}v_i$ rows and the same number of columns of $F_i$, and 
$o_{F'_i}=o_{F_i}$.
Figure~\ref{fig:splitjoin-roundrobin-matrix} illustrates how the new matrix is constructed.
We have now transformed our {\tt splitjoin} with a roundrobin splitter into a {\tt splitjoin}
with a duplicate splitter which we know how to combine already.
