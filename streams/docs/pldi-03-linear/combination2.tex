%  could replace ``filter'' by ``stream'' in the next few sentences, but 
% I think it reads better as it is, actually
\section{Combining Linear Filters}
A primary benefit of linear filter analysis is that neighboring
filters can be collapsed into a single matrix representation if both
of the filters are linear.  This transformation automatically
eliminates redundant computations in linear sections of the stream
graph, thereby allowing the programmer to write simple, modular
filters and leaving the combination to the compiler.  In this section,
we first describe a {\it linear expansion} operation that serves as a
building block for the combination techniques.  We then give rules for
collapsing {\tt pipeline}s and {\tt splitjoin}s into linear nodes; we
do not yet deal with {\tt feedbackloop}s as they require the notion of
``linear state'' (see Future Work).

\subsection{Linear Expansion}

In StreamIt, the input and output rate of each {\tt filter} in the
stream graph is known at compile time.  The StreamIt compiler
leverages this information to compute a static schedule--that is, an
ordering of the node executions such that each filter will have enough
data available to atomically execute its {\tt work} function, and no
buffer in the stream graph will grow without bound in the steady
state.  A general method for scheduling StreamIt programs is given by
Karczmarek~\cite{karczma-thesis}.

A fundamental aspect of the steady-state schedule is that neighboring
nodes might need to be fired at different frequencies.  For example,
if there are two {\tt filters} $F_1$ and $F_2$ in a {\tt pipeline} and
$F_1$ produces $2$ elements during its {\tt work} function but $F_2$
consumes $4$ elements, then it is necessary to execute $F_1$ twice for
every execution of $F_2$.

%% \begin{figure}
%% \center
%% \epsfxsize=3.0in
%% \epsfbox{images/expanding-a-filter.eps}
%% \caption{Expanding {\tt stream} $S$ by a factor $f$}
%% \label{fig:expanding-a-filter}
%% \vspace{-12pt}
%% \end{figure}

Consequently, when we combine hierarchical structures into a linear
node, we often need to {\it expand} a matrix representation to
represent multiple executions of the corresponding stream.  This
expansion can be done as follows.

% NOT TRUE ANYMORE
%% If we expand a linear node by a factor of $k$, then one execution of
%% the new node will be exactly equivalent to $k$ executions of the
%% original.

\begin{definition} (Linear expansion)
% DOUBLE-CHECK whether we're assuming 0-indexed or 1-indexed matrices
Given a linear node $\lambda = \{A, b, e, o, u\}$, the expansion of
$\lambda$ to a rate of $(e', o', u')$ is given by $\mbox{\bf
expand}(\lambda, e', o', u') = \{A', b', e', o', u'\}$, where
$\mathbf{A'}$ is a $u' \times e'$ matrix and $\mathbf{b'}$ is a
$u'$-element row vector:
\vspace{-6pt} \\
\begin{equation} \nonumber
\begin{array}{rcl}
\mathbf{A'}[i,j] & = & \mt{even}(i,j) ~\mt{if}~ (j \ge u'~\mt{mod}~u) \\
~ & = & \mt{even}(i+o, j+u'~\mt{mod}~u) ~\mt{otherwise} \\ ~ \vspace{-4pt} \\
%
\mt{even}(k)& = & \sum_{m=0}^{\lfloor u' / u \rfloor}
\mt{shift}(u'- u -m*u, \\
~ & ~ & ~ \hspace{0.68in}~ e'-e-m*o) \\ ~ \vspace{-4pt} \\
%
\multicolumn{3}{l}{\mt{shift}(r,c) ~\mt{is a}~ u' \times e' ~\mt{matrix}:} \\ ~ \vspace{-8pt} \\
\mt{shift}(r,c)[i,j] & = & A[i-r,j-c] \\
~ & ~ & ~~~~~\mt{if}~ i-r \in [0,e] \wedge j-c \in [0, u] \\
~ & = & 0 \mt{~otherwise} \\ \vspace{-4pt} \\
%
\mathbf{b'}[j] & = & b[u-1-(u'-j-1)~\mt{mod}~u]
%%
%% \raisebox{-14pt}{\parbox{3in}{is created by starting with a zero
%% matrix with $e'$ rows and $u'$ columns.  $A$ is then copied $k$ times
%% along the diagonal. Starting at the top left, each copy of $A$ is
%% offset from the previous copy by $u$ columns and $o$ rows.}} \\ ~ \vspace{-8pt} \\
%% $b'$ & is an row vector containing $k$ adjacent copies of $b$.
\end{array}
\end{equation}
\end{definition}

Despite the bulky notation above, the intuition behind linear
expansion is straightforward (see
Figure~\ref{fig:expanding-a-matrix}.)  Linear expansion aims to scale
the push, pop, and peek rates of a linear node while preserving the
functional relationship between the values pushed and the values
peeked on a given execution.  To do this, we construct a new matrix
$A'$ that contains copies of $A$ along the diagonal.  To account for
items that are popped between invocations, each copy of $A$ is offset
by $o$ from the previous copy.  The complexity of the definition is
due to the end cases.  If the new push rate $u'$ is not a multiple of
the old push rate $u$, then the last copy of $A$ includes only some of
its columns.  Similarly, if the new peek rate $e'$ exceeds that which
is needed by the diagonal of $A$s, then $A'$ needs to be padded with
zero's at the top (since it peeks at some values without using them in
the computation.)

Note that a sequence of executions of an expanded node $\lambda'$
might not be equivalent to any sequence of executions of the original
node $\lambda$, because expansion resets the push and pop rates and
can thereby modify the ratio between them.  However, if $u' = k * u$
and $o' = k * o$ for some integer $k$, then $\lambda'$ is completely
interchangebale with $\lambda$.  In the combination rules that follow,
we utilize linear expansion both in contexts that do and do not
satisfy this condition.

\begin{figure}
\center
\epsfxsize=3.0in
\epsfbox{images/expanding-a-matrix.eps}
\caption{Expanding a linear node to have I/O rates $(e', o', u')$.  }
\label{fig:expanding-a-matrix}
\vspace{-12pt}
\end{figure}

\subsection{Collapsing Linear Pipelines}

\begin{figure}
\center
\epsfxsize=3.0in
\epsfbox{images/pipeline-combination.eps}
\caption{A {\tt pipeline} of two linear forms $(A,b)$ and $(C,d)$ (above) and the same {\tt pipeline} with rate matched forms (below).}
\label{fig:combining-pipeline}
\vspace{-12pt}
\end{figure}

The {\tt pipeline} construct is used to compose streams in sequence,
with the output of stream $i$ connected to the input of stream $i+1$.
The following transformation describes how to collapse two linear
nodes in a {\tt pipeline}; it can be applied repeatedly to collapse
any number of neighboring linear nodes.

\begin{transformation} (Pipeline combination)
Given two linear nodes $\lambda_1$ and $\lambda_2$ where the output of
$\lambda_1$ is connected to the input of $\lambda_2$ in a pipeline
construct, the combination $\mbox{\bf pipe}(\lambda_1, \lambda_2)$ $=$
$\mathbf{ \{A', b', e', o', u'\}}$ represents an equivalent node that
can replace the original two.  Its components are as follows: \\
\begin{equation} \nonumber
\begin{array}{rcl}
L & = & \mt{lcm}(u_1, o_2) \\
\lambda_1^e & = & \mt{expand}(\lambda_1, \frac{o_1 * L}{u_1} + e_1 - o_1, 
\frac{o_1 * L}{u_1}, L+e_2-o_2) \\
\lambda_2^e & = & \mt{expand}(\lambda_2, L+e_2-o_2, L, \frac{u_2 * L}{o_2}) \\
\mathbf{A'} & = & A_1^e A_2^e \\
\mathbf{b'} & = & b_1^e A_2^e + b_2^e \\
\mathbf{e'} & = & e_1^e \\
\mathbf{o'} & = & o_1^e \\
\mathbf{u'} & = & u_2^e
\end{array}
\end{equation}
\end{transformation}

It is straightforward to derive the equations above.  Let $x_i$ and
$y_i$ be the input and output channels, respectively, for $\lambda_i$.
Then we have by definition that $y_1 = x_1 A_1 + b_1$ and $y_2 = x_2
A_2 + b_2$.  But since $\lambda_1$ is connected to $\lambda_2$, we
have that $x_2 = y_1$ and thus $y_2 = y_1 * A_2 + b_2$.  Substituting
the value of $y_1$ from our first equation gives $y_2 = x_1 A_1 * A_2
+ b_1 A_2 + b_2$.  Thus, the intuition is that the two-filter sequence
can be represented by matrices $A' = A_1 * A_2$ and $b' = b_1 A_2 +
b_2$, with peek and pop rates borrowed from $\lambda_1$ and the push
rate borrowed from $\lambda_2$.

However, there are two implicit assumptions in the above analysis
which complicate the equations for the general case.  First, the
dimensions of $A_1$ and $A_2$ must match for the matrix multiplication
to be well-defined.  If $u_1 \ne e_2$, this will require constructing
expanded nodes $\lambda_1^e$ and $\lambda_2^e$ in which the push and
peek rates match (and thus $A_1^e$ and $A_2^e$ can be multiplied.)

The second complication is with regards to peeking.  If the downstream
node $\lambda_2$ peeks at items which it does not consume ({\it i.e.},
if $e_2 > o_2$), then there needs to be a buffer to hold items that
are read during multiple invocations of $\lambda_2$.  However, in our
current formulation, a linear node has no concept of internal state,
such that this buffer cannot be incorporated into the collapsed
representation.  To deal with this issue, we adjust the expanded form
of $\lambda_1$ to recalculate items that $\lambda_2$ uses more than
once, thereby trading computation for storage space.  This adjustment
is evident in the push and pop rates chosen for $\lambda_1^e$: though
$\lambda_1$ pushes $\frac{u_1}{o_1}$ for every item that it pops, for
$\lambda_1^e$ this ratio is $\frac{u_1^e}{o_1^e}$ $=$
$\frac{L+e_2-o_2}{o_1 * L / u_1}$ $=$
$(\frac{u_1}{o_1})(\frac{L+e_2-o_2}{L})$, which is larger due to the
$e_2-o_2$ items that are regenerated for consumption by $\lambda_2$.

Note that although $\lambda_1^e$ performs duplicate computations in
the case where $\lambda_2$ is peeking, this redundancy disappears in
the collapsed node $\mt{pipe}(\lambda_1, \lambda_2)$ because
overlapping operations are combined in the matrix multiplication.  One
way to see this is that the dimensions of the matrix $A'$ are
independent of $e_2$, the peek rate of $\lambda_2$.  That is, $A'$ has
$e'$ $=$ $e_1^e$ $=$ $\frac{o_1*\mt{lcm}(u_1, o_2)}{u_1}+e_1-o_1$ rows
and $u'$ $=$ $u_2^e$ $=$ $\frac{u_2 * \mt{lcm}(u_1, o_2)}{o_2}$
columns; neither of these expressions depends on $e_2$.

However, it is the case that a collapsed linear node can be less
efficient than the original pipeline sequence.  The worst case is when
$A_1^e$ is a column vector and $A_2^e$ is a row vector, which requires
$O(n)$ operations originally but $O(n^2)$ operations if combined
(assuming the vectors are of length $n$).  In general, the compiler
can identify performance-degrading combinations when the number of
non-zero elements in $A'$ exceeds the sum of non-zero elements in
$A_1^e$ and $A_2^e$.
% ARE WE SURE THAT LAST SENTENCE IS TRUE?  I think it's more complicated 
% when the pop/peek rate is varied, etc.

\subsection{Collapsing Linear SplitJoins}

The {\tt splitjoin} construct allows the StreamIt programmer to
express explicitly parallel computations.  Data elements that arrive
at the {\tt splitjoin} are directed to the parallel child streams
using one of two pre-defined {\tt splitter} constructs: 1) duplicate,
which sends a copy of each data item to all of the child streams, and
2) roundrobin, which distributes items cyclically according to an
array of weights.  The data from the parallel {\tt streams} are
combined back into a single stream by means of a roundrobin {\tt
joiner} with an array of weights $w$.  First, $w_0$ items from the
output tape of the leftmost child are placed onto the overall output
tape, then $w_1$ elements are taken from the second leftmost child,
and so on.  The process repeats itself after one complete set of
$\sum_{i=0}^{N-1} w_i$ elements has been pushed.

In this section, we demonstrate how to collapse a {\tt splitjoin} into
a single linear node when all of its children are linear nodes.  Since
the children of {\tt splitjoin}s in StreamIt can be parameterized, it
is often the case that all sibling streams are linear if any one of
them is linear.  However, if a {\tt splitjoin} contains only a few
adjacent streams that are linear, then these streams can be combined
by wrapping them in a hierarchical splitjoin and then collapsing the
wrapper completely.  Our technique also assumes that each {\tt
splitjoin} admits a valid steady-state schedule; this property is
verified by the StreamIt semantic checker.

Our analysis distinguishes between two cases.  For duplicate
splitters, we directly construct a linear node from the child streams.
For roundrobin splitters, we translate the {\tt splitjoin} to use a
duplicate splitter and then rely on the first analysis to construct a
linear node.  We describe these translations below.

\subsubsection{Duplicate Splitter}

Intuitively, there are three main steps to combining a duplicate
splitjoin into a linear node.  Since the combined node will represent
a steady-state execution of the splitjoin construct, we first need to
expand each child node according to its multiplicity in the schedule.
Secondly, we need to ensure that each child's matrix representation
has the same number of rows--that is, that each child peeks at the
same number of items.  Once these conditions are satisfied, we can
construct a matrix representation for the splitjoin by simply
arranging the columns from child streams in the order specified by the
roundrobin joiner (see Figure~\ref{fig:splitjoin-duplicate-matrix}).
This third step is simplified by the fact that, with a duplicate
splitter, each row of a child's linear representation refers to the
same input element to the splitjoin.

The following transformation describes splitjoin combination in
mathematical terms.

%\begin{figure}
%\center
%\epsfxsize=3.0in
%\epsfbox{images/splitjoin-duplicate-ratematch.eps}
%\caption{Expanding sub {\tt streams} to match their output rates in a linear {\tt SplitJoin}.}
%\label{fig:splitjoin-duplicate-ratematch}
%\end{figure}

\begin{figure}
\center
\epsfxsize=3.0in
\epsfbox{images/splitjoin-duplicate-matrix.eps}
\caption{Matrix resulting from combining a {\tt splitjoin} of rate matched sub {\tt streams}.}
\label{fig:splitjoin-duplicate-matrix}
\end{figure}

\begin{transformation} (Duplicate splitjoin combination)
Given a splitjoin $s$ containing a duplicate splitter, children that
are linear nodes $\lambda_0 \dots \lambda_{n-1}$, and a roundrobin
joiner with weights $w_0 \dots w_{n-1}$, the combination $\mbox{\bf
splitjoin}(s)$ $=$ $\mathbf{ \{A', b', e', o', u'\}}$ represents an
equivalent node that can replace the entire stream $s$.  Its
components are as follows: \\
\begin{equation} \nonumber
\begin{array}{rcl}
\mt{joinRep} & = & \mt{lcm}(\frac{\mt{lcm}(u_1,w_1)}{w_1}, \dots, \frac{\mt{lcm}(u_n,w_n)}{w_n}) \\
\mt{maxPeek} & = & \mt{max}_i (o_i * \mt{rep}_i + e_i - o_i) \\ ~ \vspace{-2pt} \\
\multicolumn{3}{l}{\forall k \in [0, n-1]:} \\ ~ \vspace{-6pt} \\
\mt{wSum}_k & = & \sum_{i=0}^{k-1} w_i \\ ~ \vspace{-6pt} \\
\mt{rep}_k & = & \frac{w_k * \mt{joinRep}}{u_k} \\ ~ \vspace{-6pt} \\
\lambda_k^e & = & \mt{expand}(\lambda_k, \mt{maxPeek}, 
                              o_k * \mt{rep}_k, u_k * \mt{rep}_k) \\ ~ \vspace{-2pt} \\
\multicolumn{3}{l}{\forall k \in [0, n-1], \forall m \in [0, joinRep-1], \forall n \in [0, u_k-1]:} \\ ~ \vspace{-6pt} \\
\multicolumn{3}{l}{\mathbf{A'}[*, u'-1-n - m * \mt{wSum}_{n}-\mt{wSum}_{k}] = 
  A_{k}^e [*,u_k^e-1-n]} \\ ~ \vspace{-6pt} \\
\multicolumn{3}{l}{\mathbf{b'}[u'-1-n - m * \mt{wSum}_{n}-\mt{wSum}_{k}] = 
  b_{k}^e [u_k^e-1-n]} \\ ~ \vspace{-2pt} \\
\mathbf{e'} & = & e_0^e = \dots = e_{n-1}^e \\
\mathbf{o'} & = & o_0^e = \dots = o_{n-1}^e \\
\mathbf{u'} & = & \mt{joinRep} * \mt{wSum}_n \\
\end{array}
\end{equation}
\end{transformation}

The above formulation is derived as follows.  The $\mt{joinRep}$
variable represents how many cycles the joiner completes in an
execution of the splitjoin's steady-state schedule; $\mt{joinRep}$ is
the minimal number of cycles required for each child node to execute
an integral number of times and for all of their output to be consumed
by the joiner.  Similarly, $\mt{rep}_k$ gives the execution count for
child $k$ in the steady state.  Then, in keeping with the procedure
described above, $\lambda_k^e$ is the expansion of the $k$'th node by
a factor of $\mt{rep}_k$, with the peek value set to the maximum peek
across all of the children.  Following the expansion, each
$\lambda_i^e$ has the same number of rows, as the peek uniformization
caused shorter matrices to be padded with rows of zero's at the top.

The final phase of the transformation is to re-arrange the columns of
the child matrices into the columns of $A'$ and $b'$.
Figure~\ref{fig:splitjoin-duplicate-matrix} elucidates this process,
though the notation is somewhat cumbersome.  The equation can be
understood as follows: for the $k$'th child and the $m$'th cycle of
the joiner, the $n$'th item that is pushed by child $k$ will appear
appear at a certain location on the joiner's output tape.  This
location (relative to the start of the node's execution) is $n + m *
\mt{wSum}_n + \mt{wSum}_k$, as the reader can verify.  But since the
right-most column of each array $A$ holds the first item to be pushed,
we need to subtract this location from the width of $A$ when we are
re-arranging the columns.  The width of $A$ is the total number of
items pushed--$u'$ in the case of $A'$ and $u_k^e$ in the case of
$A_k^e$.  Hence the equation as written above: we copy all items in a
given column from $A_k^e$ to $A'$, defining each location in $A'$
exactly once.  The procedure for $b$ is analogous.

Finally, it remains to calculate the peek, pop, and push rates of the
combined node.  The peek rate $e'$ is simply $maxPeek$, which we
defined to be equivalent for all the expanded child nodes.  The push
rate $\mt{joinRep}*\mt{wSum}_m$ is equivalent to the number of items
processed through the joiner in one steady-state execution.  Finally,
for the pop rate we rely on the fact that the splitjoin is well-formed
and admits a schedule in which no buffer grows without bound.  If this
is the case, then the $\mt{pop}$ rates must be equivalent for all the
expanded streams; otherwise, some outputs of the splitter would
accumulate infinitely on the input channel of some stream.  

These input and output rates--in combination with the values of $A'$
and $b'$ defined above--define a linear node that exactly represents
the parallel combination of child nodes that are fed by a duplicate
splitter.

\subsubsection{Roundrobin Splitter}

In the case of a roundrobin splitter, items are directed to each child
stream $s_i$ according to weight $v_i$: the first $v_0$ items are sent
to $s_0$, the next $v_1$ items are sent to $s_1$, and so on.  Since a
child never sees the items that are sent to sibling streams, the items
that are seen by a given child form a periodic but non-contiguous
segment of the splitjoin's input tape.  Thus, in collapsing the
splitjoin, we are unable to directly use the columns of child matrices
as we did with a duplicate splitter, since with a roundrobin splitter
these matrices are operating on disjoint sections of the input.

Instead, we collapse linear splitjoins with a roundrobin splitter by
converting the splitjoin to use a duplicate splitter.  In order to
maintain correctness, this involves adding a decimator on each branch
of the splitjoin that eliminates items which were intended for other
streams.

\begin{transformation} (Roundrobin to duplicate)
Given a splitjoin $s$ containing a roundrobin splitter with weights
$v_0 \dots v_{n-1}$, children that are linear nodes $\lambda_0 \dots
\lambda_{n-1}$, and a roundrobin joiner $j$, the transformed
$\mbox{\bf rr-to-dup}(s)$ is a splitjoin with a duplicate splitter,
linear child nodes $\mathbf{\lambda_0'} \dots
\mathbf{\lambda_{n-1}'}$, and roundrobin joiner $j$.  The child nodes
are computed as follows: \\
\begin{equation} \nonumber
\begin{array}{rcl}
\mt{wSum}_k & = & \sum_{i=0}^{k-1} w_i \\ ~ \vspace{-6pt} \\
\mt{wTot} & = & \mt{wSum}_{n-1} \\ ~ \vspace{-4pt} \\
\multicolumn{3}{l}{\forall k \in [0, n-1]:} \\ ~ \vspace{-12pt} \\ 
\multicolumn{3}{l}{\parbox{3in}{
    \begin{equation} \nonumber
      \begin{array}{l}
      ~~~~\mt{decimate}[k] ~\mt{is a linear node}~ \{A, \vec 0, \mt{wTot}, \mt{wTot}, \mt{wTot}\} \\
	~~~~~~~\mt{where}~A[i,j] = \left\{
	\begin{array}{l}
	  1 ~\mt{if}~ i=j ~\wedge~ \\ 
	  ~\hspace{0.13in}~ wSum_{k} < i < wSum_{k+1} \\
	  0 ~\mt{otherwise}
	\end{array}
	\right.
      \end{array}
    \end{equation}}} \\
\mathbf{\lambda_k'} & = & \mt{pipe}(\mt{decimate}[k], \lambda_k)
\end{array}
\end{equation}
\end{transformation}

In the above translation, we utilize the linear pipeline combinator
$\mt{pipe}$ to construct each new child node $\lambda_i^e$ as a
composition of a decimator and the original node $\lambda_i$.  Each
decimator is a square matrix that produces and consumes $\mt{wTot}$
items, which is the number of items processed in one cycle of the
roundrobin joiner.  Those items intended for stream $i$ are copied
with a coefficient of $1$, while all others are eliminated with a
coefficient of $0$.

%% \begin{figure}
%% \center
%% \epsfxsize=3.0in
%% \epsfbox{images/splitjoin-roundrobin-matrix.eps}
%% \caption{Corresponding matrix for splitter conversion from roundrobin to duplicate.}
%% \label{fig:splitjoin-roundrobin-matrix}
%% \end{figure}
