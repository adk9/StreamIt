\clearpage
\section{Processor}

An overview of the changes to the virtual machine API appears in
Figure~\ref{fig:vmdiff}.

\begin{figure}[t]
\begin{center}
\framebox[6.5in]{
\begin{minipage}{6in}

\begin{center}
\underline{Stream Kernel API}
\end{center}

\begin{itemize}

\item {\bf The VM is object-oriented.}  Kernels, streams, and stream
graphs are all represented as objects.  Among other benefits, this
makes explicit which data are local to kernels.

\item {\bf Input and output streams are strictly typed within
kernels.}  This preserves type information and avoids casts that would
have resulted from generic STREAM\_DESCRIPTOR accesses in the previous
proposal.

\end{itemize}

\begin{center}
\underline{Stream Processor API}
\end{center}

\begin{itemize}

\item {\bf Stream objects are annotated with the node in which they
are stored.}  A stream object resides in a given memory or processor
node for its entire lifetime, and is declared with that node.

\item {\bf Static stream operations are represented as an explicit
stream graph,} in which kernels are the nodes and streams are the
edges.  The graph representation exposes parallelism and communication
patterns, and gives scheduling freedom to the low-level compiler.

\item {\bf Memory management and processor-processor 
communication are integrated into the stream graph,} as a set of
pre-defined kernels.  This replaces the previous proposal for
processor memory operations and stream network protocols.

\end{itemize}

\caption{Outline of changes to the virtual machine API.
  \protect\label{fig:vmdiff}}
\end{minipage}}
\end{center}
\end{figure}

\subsection{Threaded Processor API}

No changes.

\subsection{Stream Kernel API}

To simplify the presentation, we describe the stream kernel API before
the stream processor API.

\subsubsection{Overview}

Each kernel is now represented as a C++ object, with the following
components:

\begin{enumerate}

\item A constructor, which receives the input and output streams for
the kernel, as well as any other initialization settings.

\item A {\it work} function that represents the steady-state
execution step.

\item (Optional) A {\it prework} function that is called instead of
{\it work} on the first execution.

\item (Optional) Data members, which represent local kernel data that
are preserved between invocations of {\it work}.

\end{enumerate}

For example, a kernel for a simple amplifier could be as follows:
{\small
\begin{verbatim}
    class AmplifierKernel : Kernel_1_1 <float, float> {
      private int N;

      AmplifierKernel(istream<float> in, ostream<float> out, int _N) : Kernel_1_1(in, out) {
        N = _N
      }

      void work(istream<float> in, ostream<float> out) {
        out.push(in.pop() * N);
      }

      void work_info(istream<float> in, ostream<float> out) {
        KernelInfo.setPop(in, 1);
        KernelInfo.setPush(out, 1);
        KernelInfo.isSIMD();
      }
    }  
\end{verbatim}}
As evident in the example, input and output streams are represented by
objects that support {\tt push} and {\tt pop} operations.  In the
following sections, we describe the API for streams
(Section~\ref{sec:kerstreams}) and the API for kernels
(Section~\ref{sec:kernels}).

\subsubsection{Stream Objects}
\label{sec:kerstreams}

As in the previous proposal, a {\it stream} is a C++ data type that
represents a sequence of items of a given type.  Streams are
instantiated as part of the stream processor API, but their member
functions can only be invoked from the stream kernel API\footnote{Most
of the member functions are in {\tt istream} and {\tt ostream}, which
can only be invoked from the stream kernel API.  However, the member
functions declared in {\tt stream} must be invoked from the stream
processor API instead.}.

In this proposal, each kernel views a stream as either an {\tt
istream} or {\tt ostream}, depending on whether it is using the stream
for input or output.  An {\tt istream} provides {\tt pop}, {\tt peek},
and {\tt eos} functions, while an {\tt ostream} supports {\tt push}
and {\tt push\_eos}, as described below: {\small
\begin{verbatim}
    class base_stream {}

    template <class T>
    class istream : base_stream {
      // peek at item at position <offset> without dequeuing it
      T peek(int offset);

      // pop next item off of the stream
      T pop();

      // whether or not input is empty; that is, whether the next item
      // in the stream is an end-of-stream marker
      boolean eos();
    }

    template <class T>
    class ostream : base_stream {
      // push value onto channel
      void push(T val);

      // push an end-of-stream marker onto the stream, indicating that
      // no more data will be written.  Only needed for "source" kernels
      // that have no input streams but need to indicate completion.
      void push_eos();
    }  
\end{verbatim}}

The processor API uses the {\tt stream} class, which inherits from
both {\tt istream} and {\tt ostream}. It is described in
Section~\ref{sec:procstreams}.

\subsubsection{Kernel Objects}
\label{sec:kernels}

Each kernel is described as a subclass of a basic kernel class, such
as the following:
{\small
\begin{verbatim}
    template <class I1, class I2, class O1>
    class Kernel_2_1 {
      // construct a kernel with the given input and output streams
      Kernel(istream<I1> in1, istream<I2> in2, ostream<O1> out1);

      // steady-state execution step
      virtual void work(istream<I1> in1, istream<I2> in2, ostream<O1> out1);

      // (optional) execution step for first invocation
      void prework(istream<I1> in1, istream<I2> in2, ostream<O1> out1);

      // annotations for work function
      virtual void work_info(istream<I1> in1, istream<I2> in2, ostream<O1> out1);

      // annotations for prework function
      void prework_info(istream<I1> in1, istream<I2> in2, ostream<O1> out1);
    }  
\end{verbatim}}

The {\tt Kernel\_2\_1} base class is named as such because it contains
two input streams and one output stream.  In order to propagate type
information into the work function, the kernel is templated on the
types of the input and output streams.  Though it might be infeasible
to manually define all such base classes, the low-level compiler can
identify each appearance of Kernel\_N\_M and interpret it accordingly.

Each kernel provides a constructor in which the input and output
streams are specified.  These streams are then made available as
parameters to {\tt work} and {\tt prework}.  This is supported
automatically by the low-level compiler; no call to {\tt work} or {\tt
prework} appears in the output of the high-level compiler.

\sss{Annotations}

There are several pieces of information that are available to the
high-level compiler which should be transferred to the low-level
compiler in the form of annotations.  Each annotation takes the form
of a function call to the {\tt KernelInfo} class.  Calls from the {\tt
work\_info} function apply to {\tt work}, while calls from {\tt
prework\_info} apply to {\tt prework}.
{\small
\begin{verbatim}
    class KernelInfo {
      // these functions declare the push, pop, and maximum peek rate
      // of a work or prework function with respect to a given stream.
      static void setPush(base_stream str, int push, boolean static=true);
      static void setPop(base_stream str, int pop, boolean static=true);
      static void setPeek(base_stream str, int maxPeek, boolean static=true);

      // these functions indicate a dynamic rate that is unknown at compile
      // time.  However, they provide an optional estimate of the average
      // runtime rate to help the low-level compiler.
      static void setDynamicPush(base_stream str, float push_estimate=UNKNOWN);
      static void setDynamicPop(base_stream str, float pop_estimate=UNKNOWN);
      static void setDynamicPeek(base_stream str, float peek_estimate=UNKNOWN);

      // indicates that a work or prework function is data-parallel 
      // and fit for SIMD execution
      static void isSIMD();
    }  
\end{verbatim}}

All arguments to annotations must be compile-time constants.  For
example, in a kernel from a MergeSort, the input rates are marked
dynamic (with an estimate of 0.5 items per stream on average) because
they depend on values from the input streams:
{\small
\begin{verbatim}
    class MergeKernel : Kernel_2_1 <int, int> {
      MergeKernel(istream<int> in1, 
                  istream<int> in2, 
                  ostream<int> out) : Kernel_2_1(in1, in2, out) {}

      void work(istream<float> in1, istream<int> in2, ostream<int> out) {
        if (in1.peek(0) < in2.peek(0)) {
          out.push(in1.pop());
        } else {
          out.push(in2.pop());
        }
      }

      void work_info(istream<float> in1, istream<int> in2, ostream<int> out) {
        KernelInfo.setDynamicPush(in1, 0.5);
        KernelInfo.setDynamicPop(in2, 0.5);
        KernelInfo.setPush(out, 1);
      }
    }  
\end{verbatim}}

All streams in {\tt work} and {\tt prework} functions must be
annotated with their input and output rates.  If a peek rate is
omitted, it is assumed to be equal to the pop rate.

\sss{Kernel Restrictions}

Only a subset of C++ is supported from within a kernel; restrictions
are listed in Figure~\ref{fig:restrict}.  Note also that this proposal
does not provide special support for directly accessing streams (e.g.,
in the SRF).  Random access to a stream can be achieved by using the
{\tt peek} operation without any {\tt pop}'s.

\begin{figure}[t]
\framebox[6.5in]{
\begin{minipage}{6in}

\begin{enumerate}

\item No pointers.

\item No dynamic memory allocation.

\item No accesses to global memory.

\item No GOTO statements (all control flow is structured).

\item No recursive functions (all function calls have inline
semantics).

\item No references to templates or objects besides the classes
described in this document.  Further, no creation or casting of
objects within kernels.

\item The kernel constructor must receive all its arguments by value
(not by reference.)  Also, the kernel constructor cannot invoke any
member functions of {\tt istream} or {\tt ostream}.

\item Supported opcodes are only the logical, arithmetic, and boolean
operations found in C (no special-purpose DSP operations).

\item Supported types include 32-bit {\tt float}, 32-bit {\tt int},
16-bit {\tt short}, 8-bit {\tt byte}, {\tt boolean}, arrays with a
fixed (int literal) length, and {\tt struct}'s containing members of
any other type.

\end{enumerate}

\caption{Restrictions on C++ code within kernels.\protect\label{fig:restrict}}
\end{minipage}}
\end{figure}

\subsection{Stream Processor API}

\subsubsection{Overview}
\label{sec:streamover}

As in the previous proposal, a control thread in a restricted subset
of C++ is used to manage stream memory and to supervise the execution
of stream kernels.  However, in this proposal, all static stream
operations are represented by explicit stream graphs, in which kernel
objects are the nodes and stream objects are the edges.  The network
model is also integrated into the graph representation: each stream
object is annotated with the memory bank in which it resides, and
pre-defined network kernels are used to communicate between streams in
different memories.  Dynamic operations and dynamic control flow are
fully supported in the code that constructs stream graphs and
transitions between them.

An example of the stream processor API is as follows:
{\small
\begin{verbatim}
    // declare streams with their size and the location in which they are held
    stream<float> s_raw(1000, MEM1), s1(100, SRF1), s2(100, PROC2), s3(1000, SRF1);

    // set up a graph to do some audio filtering and compression
    Graph compress(new FileReader(s_raw, "input.dat"),
                   new Copy(s_raw, s1),
                   new FIRFilter(s1, s2),
                   new RunLengthEncode(s2, s3));

    // run the kernels on PROC2
    compress.start(PROC2);
    compress.wait();

    // if the output is still too large, run additional compression
    stream<float> s4(1000, SRF1);
    if (s3.length() > SIZE_THRESHOLD) {
      Graph compressMore(new ZipCompress(s3, s4));
      compressMore.start(PROC2);
      compressMore.wait();
    } else {
      Graph copy(new Copy(s3, s4));
      copy.start(PROC2);
      copy.wait();
    }

    // store the result to "output.dat"
    Graph store(new FileWriter(s4, "output.dat"));
    store.start(PROC2);
    store.wait();
\end{verbatim}}

The above code fragment illustrates several aspects of the stream
processor API.  In the following, Section~\ref{sec:procstreams}
describes the processor's view of the {\tt stream} type;
Section~\ref{sec:streamgraph} explains the construction of graphs from
kernels and streams; and Section~\ref{sec:predef} describes a
pre-defined set of kernels for dealing with memory and network
operations (such as the {\tt Copy} kernel above).

\subsubsection{Stream Objects}
\label{sec:procstreams}

The processor API uses the {\tt stream} class, which inherits from
both {\tt istream} and {\tt ostream}:
{\small
\begin{verbatim}
    template <class T> 
    class stream : istream <T>, ostream <T> {
      // make a stream that is buffered in units of <size> and allocated
      // at <address> of memory node <location>.  If <write_once>
      // is true, then <size> also indicates the total size of the stream,
      // and no buffering is necessary (the stream is "blocked")
      stream(int size, VM_NODE_TYPE_MEM location, int address, boolean write_once = false);

      // same as above, for stream held in registers of a processor
      stream(int size, VM_NODE_TYPE_PROC location, boolean write_once = false);

      // returns total number of elements that have been pushed to this
      // stream (this is independent of the size and write_once properties)
      int length();
    }  
\end{verbatim}}

Each {\tt stream} is constructed with a {\tt size}.  The {\tt size}
refers only to the buffering strategy of the stream--that is, how many
elements should be kept live at once before overwriting old items with
new ones.  It is the responsibility of the low-level compiler to
ensure that no items are overwritten before they are consumed.  If the
{\tt write\_once} property is true, then no overwriting is allowed,
and the {\tt size} is also an upper bound on the stream's length at
runtime.

Each {\tt stream} is also constructed with a {\tt location}, which
indicates the memory bank in which the stream is held for its entire
lifetime.  The {\tt address} indicates where the stream begins in the
memory node.  Using the second constructor, a stream can also be
assigned to a processor, which means that it is buffered in the
processor's registers.

The {\tt length} method returns how many items have been written to
the stream from the time it was constructed.  Note that this is
unrelated to the buffer size of the stream.  It is also unrelated to
how many items will be written to the stream in the future; if this
quantity is predictable, then it can be calculated as a function of
the {\tt length} of inactive streams.

\subsubsection{Stream Graphs}
\label{sec:streamgraph}

A stream graph represents a static unit of streaming computation.  It
supports the following interface:
{\small
\begin{verbatim}
    class Graph {
      // construct a graph out of any number of kernels
      Graph(void* kernel1, void* kernel2, ...);

      // Start executing the graph, with the kernels executing on
      // processor <proc>.  It will run as long as possible -- until
      // an input stream is empty.
      void start(VM_NODE_TYPE_PROC proc);

      // Blocking call to wait until the graph is finished executing,
      // or an error occurs.  A status/error code is returned.  (Can 
      // only be called after "start").
      int wait();

      // Non-blocking call to check the status of the execution.  Returns
      // a status/error code.
      int check();
    }
\end{verbatim}}

A {\tt Graph} is constructed as a set of kernels.  The connectivity of
these kernels is implicit in the stream objects that are shared
between the input of one kernel and the output of another.  The graph
does not need to be structured (as in StreamIt).  However, no two
kernels in a graph may read from (or write to) the same stream object.

Graphs are executed using the {\tt start} method, which substitutes for
the KERNELLOAD and KERNELSTART functions in the previous proposal.
The argument to {\tt start} specifies which stream processor should
execute the arithmetic operations of the graph; note that this can be
the same processor that is running the control code.

The {\tt start} method runs each kernel ``as long as possible''.  For
kernels with static input rates, this means that the kernel is fired
atomically until there are fewer items on an input channel than is
required by the kernel.  For kernels with a dynamic input rate, this
means that the kernel is run until either 1) it attempts to peek or
pop an item beyond the end-of-stream marker, or 2) it executes once
without consuming or producing any items.

The {\tt wait} and {\tt check} methods replace the VM\_DONE\_SYNC and
VM\_DONEQUERY functions of the previous proposal.  Since the {\tt
start} function is asynchronous, these methods are needed to inspect
the status of graph execution.  The {\tt wait} method blocks until the
graph has finished, while {\tt check} returns the status immediately.

\sss{Transitioning Between Graphs}

A {\tt Graph} represents only the static sections of the stream graph.
As in the example of Section~\ref{sec:streamover}, dynamic control
flow can surround the construction of graphs and can predicate their
execution.  In addition, streams from one graph can be reused in
subsequent graphs.  This is essential for carrying over results from
one streaming computation to another.  In the example, streams {\tt
s3} and {\tt s4} are used in multiple graphs.  Streams can also be
declared in a global namespace if they need to be shared between
multiple processors.

It is also possible for the processor API to inspect the public
fields of a kernel following a streaming computation.  This could be
useful in passing parameters to subsequent stream graphs, or for
retrieving a reduction value from inside a kernel.  For instance:
{\small
\begin{verbatim}
    // --- kernel code ---
    class SumKernel : Kernel_1_0 <int> {
      public int sum;

      SumKernel(istream<int> in) : Kernel_1_0 (in) {
        sum = 0;
      }

      void work(istream<int> in) {
        sum += in.pop();
      }

      void work_info(istream<int> in) {
        KernelInfo.setPop(in, 1);
      }
    }

    // --- processor code ---
    stream<int> s1(100, SRF);

    SumKernel sk(s1);
    Graph g(new FileReader(s1, "input.dat"), sk);
    g.start(PROC1);
    g.wait();

    int final_sum = sk.sum;
\end{verbatim}}

There seems to be nothing fundamental to prevent the stream processor
API to mutate kernel fields as well as inspect them, but at this point
we disallow this since (for performance reasons) most mutation should
be done from within the kernel itself.

\sss{Restrictions on Stream Control}

We refer to a toplevel procedure that controls the execution of stream
kernels as a {\it stream control function}.  Generally a stream
control function will be spawned as a thread and assigned to a control
processor from the main thread of an application.

In order to facilitate static analysis in the low-level compiler,
there are a number of restrictions on the code that can appear in a
stream control function.  These are as follows:

\begin{enumerate}

\item The restrictions of stream kernels (see
Figure~\ref{fig:restrict}) also apply to stream control functions,
with the following caveats:

\begin{itemize}

\item Global memory is not directly addressable from stream control
functions, but results can be returned via arguments passed by
reference.

\item Recursive functions are allowed if they don't contain any
reference to a stream, kernel, or graph object.

\end{itemize}

\item No methods of class {\tt istream} or {\tt ostream} can be
invoked from a stream control function.

\item All stream, kernel, and graph variables have a static single
assignment in the program.  This exposes exactly which constructor is
invoked for a given variable name.

\item All streams must be declared with a size that is constant ({\it
i.e.,} the size argument to the stream constructor must be an {\tt
int} literal).

\item All assignments of kernels to processors must be constant ({\it
i.e.,} the processor argument to the {\tt graph.start} method must be a
constant literal).

\item $(*)$ A kernel can appear in only one graph.  That is, on every
execution path, a given kernel object is used as an argument to at
most one {\tt Graph} constructor.

\item $(*)$ Each graph can only be run once.  That is, on every
execution path, a given graph object is the target of at most one {\tt
start} call.

\item $(*)$ Only one graph can be run at a given time on a given
processor.  That is, for each processor $P$, there can be at most one
unfinished graph object that was run with a call of {\tt
start(}$P${\tt )}.

\end{enumerate}

The $(*)$ items represent dynamic properties that are not statically
verifiable by the low-level compiler.  Thus, they represent a contract
that the high-level compiler must respect for the sake of correctness.

\subsubsection{Pre-Defined Kernels}
\label{sec:predef}

This proposal integrates all memory management and network support for
streams into the graph model of the previous section.  This is done
using pre-defined kernels that can connect streams in different
memories, or route streams to network channels.

\subsubsection*{Memory Management Kernels}

The memory management kernels correspond directly to one or more of
the functions that the previous proposal included in the stream
processor API.

\ssss{Copy} The {\tt Copy} kernel substitutes for the load, store, and move
functions of the previous proposal.
{\small
\begin{verbatim}
    template <class I1, class O1>
    class Copy : Kernel_1_1 <I1, O1> {
      Copy(istream<I1> src_str, ostream<O1> dest_str, int length = ENTIRE_STREAM,
           int src_offset = 0, int dest_offset = 0, int record_size = 1, int stride = 1);
    }
\end{verbatim}}

The primary use of this kernel is for copying items between streams in
different memory banks, though it can also be used for copying between
streams in a single memory.  However, it can only copy across one
connection--in the architectural graph, there must be an edge from the
location of the input stream to the location of the output stream.
(When the source stream is located in main memory, this corresponds to
the previous notion of a load; when the source stream is located in
the SRF, this corresponds to the previous notion of a store.)

As in the previous proposal, the {\tt record\_size} indicates the
number of words in each data record, and the {\tt stride} indicates
the separation between records in the source stream.  The {\tt length}
indicates how many words should be transferred; by default, all
elements in the {\tt src\_str} are copied.  The {\tt src\_offset} and
{\tt dest\_offset} arguments can be used for shifting all accesses by
an offset in the source or destination streams.  

We omit the optional {\tt \_op} allowed by the previous proposal (for
in-place memory updates), because we believe that the new structure of
the stream graph will clearly expose these optimization opportunities.

\ssss{Scatter/Gather} The {\tt Scatter} and {\tt Gather} kernels are
very similar to the functions in the previous proposal.
{\small
\begin{verbatim}
    template <class I1, class I2, class O1>
    class Scatter : Kernel_2_1 <I1, I2, O2> {
      Scatter(istream<I1> src_str, istream<I2> index_str, ostream<O1> dest_str, 
              int src_offset=0, int dest_offset=0, int record_size = 1);
    }

    template <class I1, class I2, class O1>
    class Gather : Kernel_2_1 <I1, I2, O2> {
      Gather(istream<I1> src_str, istream<I2> index_str, ostream<O1> dest_str, 
             int src_offset=0, int dest_offset=0, int record_size = 1);
    }  
\end{verbatim}}

These kernels copy items from a source stream to a destination stream,
in chunks of {\tt record\_size} words.  In the {\tt Scatter} kernel,
the {\tt index\_str} indicates the positions in the output stream at
which the records should be written; in the {\tt Gather} kernel, the
{\tt index\_str} indicates the positions in the input stream at which
the records should be read.  The {\tt src\_offset} and {\tt
dest\_offset} arguments can be used for shifting all accesses by an
offset in the source or destination streams.

Like the {\tt Copy} kernel, these kernels assume that the
architectural graph contains an edge from the location of the input
stream to the location of the output stream.  The location of the
index stream can be on either of the two nodes.

\subsubsection*{Network Kernels}

The network kernels are for processor-processor communication.  They
serve as a substitute for the previous proposal's network management
of streams.

\ssss{Send} The {\tt Send} kernel sends a stream from one processor to
another, subject to the connection protocol described below.
{\small
\begin{verbatim}
    template <class I1>
    class Send : Kernel_1_0 <I1> {
      // send <src_str> on <channel> of <connection>
      Send(istream<I1> src_str,  VM_EDGE connection, int channel);
    }
\end{verbatim}}

Given that the kernel is executing on processor $P$, we require that
{\tt src\_str} is located in a memory connected to $P$ (or located
within $P$ itself), and that {\tt connection} is an edge from $P$ to a
neighboring processor.

\ssss{Receive} The {\tt Receive} kernel receives a stream from a
neighboring processor, subject to the connection protocol described
below.  
{\small
\begin{verbatim}
    template <class O1>
    class Receive : Kernel_0_1 <I1> {
      // receive <dest_str> from <channel> of <connection>.
      Receive(ostream<O1> dest_str,  VM_EDGE connection, int channel);
    }  
\end{verbatim}}

Given that the kernel is executing on processor $P$, we require that
{\tt dest\_str} is located in a memory connected to $P$ (or located
within $P$ itself), and that {\tt connection} is an edge into $P$ from
a a neighboring processor.

\ssss{Send/Receive Protocol} We refer to channel number $n$ of
connection $c$ as the pair $(c, n)$.  Note that $n$ is a virtual
channel identifier; $n$ does not need to fall within $[0,
\mbox{VM\_PROP\_CHAN\_NUM}]$ for connection $c$.  Rather, the
communication protocol will ensure that there are less than
VM\_PROP\_CHAN\_NUM active channels at a time.

The protocol maintains a queue of {\tt Send} and {\tt Receive} kernels
that are waiting to communicate across each $(c, n)$; let them be
$\mt{SendQ}(c, n)$ and $\mt{ReceiveQ}(c, n)$, respectively.  Kernels
are pushed onto these queues in the same order that their containing
graphs are executed from the stream processor API.  We disallow the
case where multiple kernels in a given graph are targeting the same
queue.  Thus, the order of the kernels in the queues is
well-defined\footnote{Unless there are multiple threads executing on
the control processor, in which case synchronization should be used to
ensure a deterministic ordering of the send/receive kernels across
threads.}.

To open a new session of data transfer across $(c, n)$, the following
conditions must be met:
\begin{enumerate}

\item Channel $n$ is {\it free} on connection $c$.  That is, no other
session is open on $(c, n)$, and $c$ has room for another active
channel.

\item $\mt{SendQ}(n,c)$ and $\mt{ReceiveQ}(c, n)$ are non-empty.

\end{enumerate}
If these conditions are satisfied, then a new session is opened
between the kernels at the front of $\mt{SendQ}(c, n)$ and
$\mt{ReceiveQ}(c, n)$.  Items are transmitted across the channel until
the graph containing the {\tt Send} kernel finishes its execution.  At
this point, an end-of-stream marker is inserted into the {\tt
dest\_str} of the {\tt Receive} kernel, the session is terminated, and
both the {\tt Send} and {\tt Receive} kernels are removed from the
respective queues for $(c, n)$.

Note that until a session is opened, all pending kernels are blocked.
The graphs that contain these kernels could possibly execute other
nodes, but the {\tt Send} or {\tt Receive} nodes must wait until the
channel is ready.

\subsubsection*{Example}

We consider one more example to illustrate the use of the above
kernels.  In this example, there are two processors that each contain
their own memory:

\begin{figure}[h]
\begin{center}
\psfig{figure=ex1.eps,width=2in}
\end{center}
\vspace{-12pt}
\end{figure}

The application does audio segmentation on a series of 10 input files
and plays a summary of each file on a speaker.  The first processor
does the segmentation itself, while the second processor filters the
extracted segments to provide a smooth transition between them.

{\small
\begin{verbatim}
    // --- code for PROC1 ---

    for (int i=0; i<10; i++) {

      stream<float> raw_data(10000, MEM1, true), spectrum(100, MEM1),
                    sim(10, PROC1), seg_indices(10, PROC1), sum_data(10, PROC1);

      Graph g(new FileReader(raw_data, filename[i]),       // load file
              new FFT(raw_data, spectrum, N),              // extract spectrum
              new SimilarityMatrix(spectrum, sim),         // detect local similarity
              new ExtractSegments(sim, seg_indices),       // make indices of summary segments
              new Gather(raw_data, seg_indices, sum_data), // gather summary audio in sum_data
              new Send(sum_data, c1, 3));                  // send over connection c1, channel 3

      g.start(PROC1);                                      // run for whole length of file
      g.wait();
    }

    // --- code for PROC2 ---

    for (int i=0; i<10; i++) {

      stream<float> sum_data(100, MEM2), smooth_data(100, MEM2);

      Graph g(new Receive(sum_data, c1, 3),                // receive summaries over channel
              new FIRFilter(sum_data, smooth_data),        // filter summaries
              new Speaker(smooth_data));                   // send to speaker

      g.start(PROC2);
      g.wait();

    }   
\end{verbatim}}
In processor 1, a {\tt Gather} kernel is used to load the audio file
at the indices where the summary segments appear.  The {\tt Gather}
kernel is directly connected to a {\tt Send} kernel which sends the
summary segments across virtual channel 3 of connection {\tt c1}.
Processor 2 uses a {\tt Receive} kernel to receive the summary
segments before filtering them and sending them to a speaker.  Note
that there are 10 sessions of data transfer between the processors,
and the amount of data transferred during each session depends on the
length of the audio file; a session is terminated when processor 1
finishes executing its stream graph.

\section{Memory}

With regards to the threaded VM, there are no changes to the memory
API.  As described above, this proposal allows streaming memory access
only via stream objects.  The layout of stream objects to specific
memory addresses is not done by the high-level compiler.

\section{Network}

With regards to the threaded VM, there are no changes to the network
API. As described above, this proposal allows streaming network access
only through a pre-defined set of kernels.
