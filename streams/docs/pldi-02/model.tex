%% \begin{figure}
%% \centering
%% \psfig{figure=tapes.eps,width=2.5in}
%% \caption{A filter's input and output tapes during an execution step.
%% With each step, the filter pushes two items, pops two items, and peeks
%% at three additional items.  The initial state of the input tape is
%% shown at left.  The center shows the filter with both input and output
%% tapes during the invocation of {\tt work}.  The final state of the
%% output tape is shown at right.}
%% \label{fig:tapes}
%% \end{figure}

\begin{figure}
\centering
\psfig{figure=pipeline.eps,width=1.64in}

(a) A Pipeline. \\
\vspace{6pt}
\psfig{figure=splitjoin.eps,width=2.75in}

(b) A SplitJoin. \\
\vspace{6pt}
\psfig{figure=feedback.eps,width=2.17in}

(c) A FeedbackLoop. \\
\vspace{-6pt}
\caption{StreamIt structures with labeling.}
\vspace{-12pt}
\label{fig:tapelabels}
\end{figure}

\section{Streaming Model of Computation}

In this section, we develop an abstract model of streaming computation
to serve as a basis for reasoning about program transformations and
compilation techniques within the streaming domain.  A stream graph
differs from a traditional, sequential program in that all of the
filters of the graph are implicitly running in parallel, with the
execution order constrained only by the availability of data on
channels between the filters.  Further, filters communicate only with
their immediate neighbors, thereby removing any notion of global time
or non-local dependences of one filter on another.  [add idea that it
is the specification of the atomic work function that really prevents
global time] These properties merit the development of a new model of
computation, in which the notions of timing, scheduling, and
dependence analysis are in terms that are relative to a given filter
in the graph, instead of being global characteristics of a program.

In Section \ref{sec:minfunc}, we develop a transfer function that
provides the basis for distributed time in a stream graph... [build
operational semantics to give a precise meaning to messaging, and
denotational semantics to validate program transformations].

\subsection{Notation}

We use the following notation:

\begin{itemize}

\item A {\it tape} is an infinite history of the values that have been
  pushed onto a channel between two filters. We use $I_S$ and $O_S$ to
  denote the input and output tapes of stream $S$, respectively, with
  numbering used to distinguish between multiple input or output tapes
  (see Figure \ref{fig:tapelabels}).  Finally, $n(T)$ represents the
  number of items on tape $T$ at a given point of execution.  [should
  we define $p(T)$ here or wait until we use it?  long time def-use!]

\item We say that a filter $A$ is {\it upstream} of filter $B$ (or,
  equivalently, $B$ is {\it downstream} $A$) if there is a directed
  path in the stream graph from $O_A$ to $I_B$.  We use this
  terminology for tapes as well as filters.

\item The number of items that are pushed, popped, and peeked by
  filter $A$ during a single execution of its work function are
  denoted by $push_A$, $pop_A$, and $peek_A$, respectively.  Note that
  $peek_A$ includes the items that are popped, such that $pop_A \le
  peek_A$.

\end{itemize}

\subsection{Relative Time}
\label{sec:minfunc}

As outlined above, there is no concept of global time in a stream
graph since each filter is completely independent and can only
communicate with its neighbors through input and output channels.
Thus, if two filters need to synchronize an event, the synchronization
must be in terms of the data items that are passed over a channel.

In this context, we define a $min$ function between tapes in the
stream graph that allows disconnected filters to have a common notion
of time.  The function is defined in terms of data dependence:
\begin{definition}
$\mi{a}{b}(x)$ is the minimum number of items that must appear on tape
$a$ given that there are $x$ items on tape $b$.
\end{definition}
We now turn to deriving $\mi{a}{b}$ for all pairs of tapes $a$ and $b$
in a filter graph where $a$ is upstream of $b$.

\subsubsection{Filters}

Let us derive $\mi{I_A}{O_A}(x)$, which represents the time shift
across a single filter $A$.  Since the filter produces $push_A$ items
on every invocation, it must be invoked
$\left\lceil\frac{x}{push_A}\right\rceil$ to produce the $x$'th item.
On each invocation, it consumes $pop_A$ items, and peeks at an
additional $peek_A-pop_A$ items.  Thus, the total number of items that
must be present on the input is:
\begin{align}
\label{eq:minfilter}
\mi{I_A}{O_A}(x) = \left\lceil\frac{x}{push_A}\right\rceil*pop_A+(peek_A-pop_A)
\end{align}

\subsubsection{Pipelines}
\label{sec:timepipe}

Let us now derive an expression for $min$ in the case of a pipeline.
In the base case, consider that two filters are connected, with the
output of $A$ feeding into the input of $B$ (see
Figure~\ref{fig:tapelabels}).  We are seeking $\mi{I_A}{O_B}(x)$: the
minimum number of items that must appear on tape $I_A$ given that
there are $x$ items on tape $O_B$.  Observing that a minimum of
$\mi{I_B}{O_B}(x)$ items must appear on tape $I_B$, and that $I_B$
must equal $O_A$ since the filters are connected, we see that a
minimum of $(\mi{I_A}{O_A} \circ \ma{I_B}{O_B})(x)$ items must appear
on $I_A$:
\begin{align*}
\mi{I_A}{O_B} = \mi{I_A}{O_A} \circ \mi{I_B}{O_B}
\end{align*}
By identical reasoning, this composition law holds for pipelined
streams as well as filters.  That is, a Pipeline of streams $S1 \dots
Sn$ has the following $min$ function:
\begin{align}
\label{eq:composepipe}
\mi{S1}{Sn} &= \mi{I_{S1}}{O_{S1}} \circ \dots \circ \mi{I_{Sn}}{O_{Sn}}
\end{align}
One might be tempted to define the $min$ function for any pair of
connected tapes as the composition of functions for the operators
connecting those tapes.  However, such a definition turns out to be
problematic for the SplitJoin and FeedbackLoop constructs, which
require a slightly different composition law for their components (as
shown below).  Instead, we can further extend our notation to include
the {\it components} of streams that are connected in a pipeline.
That is, if tapes $t_i$ and $t_j$ are contained within stream
constructs $S_i$ and $S_j$, respectively, and $S_i$ and $S_j$ belong
to a pipeline of streams $S_1 \dots S_n$, then:
\begin{align}
\label{eq:composetape}
\mi{t_i}{t_j} &= \mi{t_i}{O_{S_i}} \circ \mi{I_{S_{i+1}}}{O_{S_{i+1}}}
\circ \dots \circ \mi{I_{S_j}}{t_j}
\end{align}

\subsubsection{SplitJoins}
\label{sec:timesj}

We now derive $min$ expressions for the components of a SplitJoin, and
for the SplitJoin construct as a whole.  We denote the $n$ output
tapes of the splitter $S$ by $O1_S \dots On_S$, and the $n$ input
tapes of the joiner $J$ by $I1_J \dots In_J$ (see Figure
\ref{fig:tapelabels}).

{\bf Duplicate splitter.}  We consider the $i$'th output tape of an
$n$-way duplicating splitter.  Since the splitter duplicates each
input item onto each output tape, there must be at least $x$ items on
$I_S$ if there are $x$ items on $Oi_S$.  This yields a simple
expression for $min$:
\begin{align*}
\mi{I_S}{Oi_S}(x) = x
\end{align*}

{\bf Weighted round robin splitter.}  Let us consider an $n$-way
splitter with weights $w_1 \dots w_n$.  Observe that if there are
$n(On_S)$ items on the $n$'th output tape, then the splitter must have
executed $\floor{n(On_S)}{w_n}$ complete cycles in distributing items
to the output tapes; each cycle draws $sum_{i}{w_i}$ items from the
input tape $I_S$.  Further, if there are $n(Oi_S)$ items on the $i$'th
output tape, then $n(Oi_S) mod w_i$ additional items have been
deposited on $Oi_S$ during the current cycle of the splitter, and
$n(Oi_S)~mod~w_i + sum_{j=0}^{i-1}{w_j}$ items have been drawn from
the input since the last complete cycle.  Summing the item count for
the completed cycles and the current cycle gives the following
expression for $min$:
\begin{align*}
\mi{I_S}{Oi_S}(x) = \floor{n(On_S)}{w_n} * \sum_{i}{w_i} + x~mod~w_i +
\sum_{j=0}^{i-1}{w_j}
\end{align*}

{\bf Weighted round robin joiner.}  The reasoning is similar for an
$n$-way joiner with weights $w_1 \dots w_n$.  Let us use $W$ to denote
the sum of the weights: $W = \sum{i=1}^{n}{w_i}$.  If there are $x$
items on the output tape $O_J$, then the joiner must have executed
$\floor{x}{W}$ complete cycles, each of which drew $w_i$ items from
the $i$'th input tape.  Since the last complete cycle, the joiner has
drawn $x~mod~W$ items from its inputs, and $MIN(0, x~mod~W -
\sum_{j=0}^{i-1}{w_j})$ of these items were taken from input tape $j$.
Thus, the $min$ function from the output of the joiner to the $i$'th
input tape is as follows:
\begin{align*}
\mi{Ii_J}{O_J}(x) = w_i * W + MIN(0, x~mod~W - \sum_{j=0}^{i-1}{w_j})
\end{align*}

{\bf SplitJoin construct.}  As with the Pipeline construct, we can
derive the $min$ function across an entire SplitJoin as a composition
of the component functions.  However, a SplitJoin differs from a
Pipeline in that the joiner imposes a control dependence between the
parallel streams.  That is, for there to be $x$ items on the output of
the joiner, there must be at least $\mi{Ii_J}{O_J}(x)$ items on {\it
every} input tape $Ii_J$.  Applying the composition law for pipelines
(Equation \ref{eq:composepipe}), it follows that there must be at
least at least $\mi{Oi_S}{Ii_J} \circ \mi{Ii_J}{O_J}(x)$ items on
every output tape $Oi_S$ of the splitter.  Finally, the minimum number
of items appearing on the input tape $I_S$ of the splitter is the {\it
maximum} of the item requirement from any output tape $Oi_S$.  By this
reasoning, the $min$ function for a SplitJoin is as follows:
\begin{align}
\mi{I_S}{O_J}(x) &= MAX_{i \in [1,n]}(\mi{I_S}{Oi_S} \circ
\mi{Oi_S}{Ii_J} \circ \mi{Ii_J}{O_J})(x)
\end{align}

\subsubsection{FeedbackLoops}
\label{sec:timefl}

The $min$ function for a feedback loop requires extra care. Although
the feedback splitter $FS$ serves as a normal splitter, with the same
$min$ function as derived above, the feedback joiner $FJ$ is slightly
different due to the initialization phase of the loop.  Also, the
$min$ function does not compose across all components of the loop,
since otherwise there would be conflicting definitions for paths that
circle the loop several times.

{\bf Feedback joiner}.  For a feedback loop with delay $d$, the
feedback joiner must fabricate its first $d$ input values, since no
items have yet been pushed onto the loop tape $I2_{FJ}$.  This means
that there must be an offset of $d$ in the $min$ function, since the
first $d$ items are direct inputs to the joiner instead of appearing
as items on the input tape.  Using $J$ to denote a weighted round
robin joiner as considered above, we thus have the following
expression for the $min$ function across the feedback path:
\begin{align*}
\mi{I2_{FJ}}{O_{FJ}}(x) = \mi{I2_J}{O_J}(x) - d
\end{align*}
However, the $min$ function remains unchanged with respect to the
input from the main stream:
\begin{align*}
\mi{I1_{FJ}}{O_{FJ}}(x) = \mi{I1_J}{O_J}(x)
\end{align*}

{\bf Feedback components}.  Within a feedback loop, the $min$ function
between tape $a$ and any downstream tape $b$ can be uniquely defined
by composing the $min$ functions along the directed acyclic path
between $a$ and $b$.  We require an acyclic path to avoid successive
passes around the loop, which would prevent a unique definition of the
function.  Denoting this path of tapes by $(a, t_1, \dots , t_n, b)$,
the composition follows the form of Equation \ref{eq:composepipe}:
\begin{align*}
\mi{a}{b}(x) = \mi{a}{t_1} \circ \mi{t_1}{t_2} \circ \dots \mi{t_n}{b}
\end{align*}
Note that these functions can then be composed with those of
constructs neighboring the feedback loop to obtain, for instance, the
relation between the loop tape $I2_{FJ}$ and a downstream pipeline (by
application of Equation \ref{eq:composetape}).

{\bf Feedback loop construct.}  As a special case of the equation
above, we can see that the $min$ function for the feedback loop as a
whole is the composition of the $min$ functions along the main path:
\begin{align*}
\mi{I1_{FJ}}{O_{FS}}(x) = \mi{I1_J}{O_J}(x) \circ \mi{I1_J}{O_J}(x) 
\end{align*}
Intuitively, this is because--in any semantically correct stream
program--the loop itself is guaranteed to have enough inputs to feed
the joiner, such that the output tape of the feedback loop places a
restriction only on the input tape of the feedback loop.

\subsubsection{Summary}

In the preceding sections, we have derived a $min$ function for the
components of each stream construct, as well as for the stream
construct as a whole.  By application of Equation
\ref{eq:composetape}, this yields a function $\mi{a}{b}$ for every
pair of tapes $a$ and $b$ where $b$ is downstream of $a$.

[say some more profound stuff about the min function?  information
  flow, NOT data dependence.  The data dependence is in the work
  functions themselves; this is a property of the graphs.  distributed
  time.]

\subsection{Program Verification}
\label{sec:prog-verif}

A number of program analysis techniques are immediately afforded by
the $min$ function.  In particular, it is very simple to compute 1)
whether or not the program will deadlock as a result of a starved
input channel, and 2) whether or not any buffer will grow without
bound during the steady-state execution of the program.

{\bf Deadlock detection.}  The deadlock detection algorithm takes
advantage of the fact that the only loops in our stream graph are part
of a FeedbackLoop construct.  A stream graph will be deadlock-free if
and only if every feedback loop produces enough data to satisfy its
own feedback joiner.  This can be formulated in terms of the $min$
function by considering $min_{t}{t}$, the data that a tape $t$ in a
feedback loop requires of itself.  However, since we didn't define
$min$ across circular paths in the stream graph, we will denote this
function by $minloop$ and define it at the loop input to the feedback
joiner:
\begin{align*}
minloop(x) \equiv \mi{I2_{FJ}}{O_{FJ}} \circ \mi{O_{FJ}}{I2_{FJ}}
\end{align*}
Now, the loop will be deadlock-free if and only if $\forall x \in N, x
- minloop(x) > 0$.  This condition follows directly from
causality--the $x$'th item can be produced if and only if its
production depends only on some subset of the $x-1$ items that are
already on the channel.

{\bf Overflow detection.}  There are two places that a buffer can grow
to an unbounded size in the stream graph.  The first is in a feedback
loop, when\footnote{$f(x) = \omega(g(x))$ if $lim_{x \rightarrow
\infty}\frac{f(x)}{g(x)} = \infty$}~$x - minloop(x) = \omega(1)$.
That is, if $minloop(x)$ items on the feedback tape enables the
production of an additional $x - minloop(x)$ items that grows
asymptotically with the position $x$ on the tape, then the constant
consumption rate will not keep up with the growing production rate,
and the buffer will overflow.

The second case of buffer overflow is when the parallel streams of a
SplitJoin have asymptotically different production rates.  For a given
stream $i$ in a SplitJoin construct, the buffer corresponding to the
joiner input tape $Ii_J$ will overflow if and only if there is a
stream $j$ in the SplitJoin for which:
\begin{align*}
(\mi{I_S}{Oi_S}& \circ \mi{Oi_S}{Ii_J} \circ \mi{Ii_J}{O_J})(x) - \\
(\mi{I_S}{Oj_S}& \circ \mi{Oj_S}{Ij_J} \circ \mi{Ij_J}{O_J})(x) = \omega(1)
\end{align*}
Both of these cases could be detected by a compiler to verify that no
buffers will overflow during steady-state execution.

\subsection{Information Flow}

In the above sections, the $min$ function is mostly described in terms
of data dependences.  However, we can also think of this function as
defining a common timing mechanism that asynchronous filters can use
to synchronize events.  We present this timing mechanism in terms of
``information flow'', which we believe is a central concept of the
streaming domain.  Using this concept, we give a precise semantics to
message delivery timing in in StreamIt.

\subsubsection{Information Wavefronts}

When an item enters a stream, it carries with it some new information.
As execution progresses, this information cascades through the stream,
effecting the state of filters and the values of new data items which
are produced.  We refer to an ``information wavefront'' as the set of
filter executions that first sees the effects of a given input item.
Thus, although each filter's {\tt work} function is invoked
asynchronously without any notion of global time, two invocations of a
work function occur at the same ``information-relative time'' if they
operate on the same information wavefront.

The $min$ function can be used to give a precise definition to an
information wavefront.  One interpretation of $y = \mi{a}{b}(x)$ is
that the item at position $y$ of tape $a$ is the the latest item on
tape $a$ to {\it effect} the item at position $x$ of tape $b$.  This
is because item $x$ on tabe $b$ can be produced if and only if tape
$a$ contains at least $y$ items.  Note that this effect might be via a
control dependence rather than a data dependence--for instance, if
item $y$ needs to pass through a round-robin joiner before some data
from another stream can be routed to tape $b$.  This is why we choose
``information flow'' instead of ``data flow'' to describe the timing
concept.  Also, when speaking of the information wavefront, we only
consider information that is passed through the data streams; if a
data item effects another via a low-latency downstream message, then
this effect could jump ahead of the wavefront.

\subsubsection{Message Timing}
\label{sec:messagesemantics}

We now employ the $min$ function to give a precise meaning to the
message delivery guarantees in StreamIt.  Suppose that filter $A$
sends a message to filter $B$ with latency $\lambda$, where $\lambda$
is any integer.  After $\lambda$ invocations of $A$'s {\tt work}
function, $A$ will produce (or has produced) one or more data items
$d$.  Now, the messaging system guarantees that:
\begin{enumerate}

\item If $B$ is upstream of $A$, then $B$ will receive the message
immediately following the last invocation of its {\tt work} function
which produces items that affect $d$.

\item If $B$ is downstream of $A$, then $B$ will receive the message
immediately preceding the first invocation of its {\tt work} function
which produces items that are effected by $d$.

\end{enumerate}
Now let us express these constraints in terms of the $min$ function.
Letting $s$ be equal to $n(O_A)$ at the time that the message was
sent, we have that:
\begin{enumerate}

\item If $B$ is upstream of $A$, the message will be delivered when:
\begin{align}
\label{eq:msgup}
n(O_B) = push_B * \ceil{\mi{O_B}{O_A}(s + push_A * \lambda)}{push_B}
\end{align}
That is, $s + push_A * \lambda$ is the number of items on $A$'s output
tape after producing the data of interest.  Then, $y = \mi{O_B}{O_A}(s
+ push_A * \lambda)$ is the latest item on $B$'s output tape that
affects the data of interest.  The message should be delivered
immediately after the work function producing this item, which occurs
when the item count $n(O_B)$ equals $push_B * \ceil{y}{push_B}$, as
specified by the constraint.

\item If $B$ is downstream of $A$, the message will be delivered when:
\begin{align}
\label{eq:msgdown}
n(O_B) = MAX(x~s.t.~\mi{O_A}{O_B}(x) = s + push_A * (\lambda-1))
\end{align}
That is, $z = s + push_A * (\lambda - 1)$ is the number of items on
$A$'s output tape before pushing the data of interest.  The message
should be delivered when $B$ has executed as far as possible given
that there are $z$ items on $O_A$.  That is, $B$ should push $x$ items
on its output tape for the maximum $x$ that still depends on some of
the $z$ items, which is given by the condition $\mi{O_A}{O_B}(x) = z$
above.

Thus, when $A$ pushes the next set of data, it could affect the data
that will be pushed next onto the output tape of $B$.  (Note that the
next set of data from $A$ might not be sufficient to calculate the
next set on $B$'s output, but it could affect it nonetheless.)  The
message must be delivered immediately before this effected data
appears on $B$'s output, so the number of items $n(O_B)$ is as given
above.  We do not need a ceiling function as in (1) because we are
guaranteed to have the maximum $x$ at a multiple of $push_B$.

\end{enumerate}

\subsubsection{Latency Constraints}

The framework describe above can be used to give a semantics to the
latency constraints in StreamIt.  Each directive MAX\_LATENCY(A, B, n)
has the same effect as defining a message from filter $B$ to upstream
filter $A$ with latency $n$.

\subsection{Operational Semantics}

We can fully define the possible sequences of filter executions as a
set of constraints on the number of items on each tape in the stream
graph.  This is useful not only from the perspective of semantics, but
for compiler analysis of the space of valid schedules.  To begin the
analysis, we formulate the constraints imposed by message delivery
guarantees on the number of items on each tape.

Suppose that a filter $A$ might send a message to filter $B$ with a
maximum latency of $\lambda$ during any invocation of its work
function.  Then we must constrain the execution of $B$ to make sure
that it is not too far ahead to receive the message with the given
latency.  That is, we can only execute $B$ so long as $n(O_B)$--the
item count on its output tape--does not exceed the count when a
message would be delivered.  Recalling the expression for message
delivery time (Equations \ref{eq:msgup} and \ref{eq:msgdown}), this
constraint is as follows if $B$ is upstream of $A$:
\begin{align}
\label{eq:mc1}
n(O_B) \le push_B * \ceil{\mi{O_B}{O_A}(n(O_A) + push_A * \lambda)}{push_B}
\end{align}
and as follows if $B$ is downstream of $A$:
\begin{align}
\label{eq:mc2}
n(O_B) \le MAX(x~s.t.~\mi{O_A}{O_B}(x) = n(O_A) + push_A * (\lambda-1))
\end{align}
The guarantees for latency are treated identically to message
guarantees, as fitting with the semantics of latency as described
above.

{\bf Defining the schedule.}  We now have a set of constraints
expressing whether or not a given set of tapes respects the latency
and message delivery guarantees in a program.  We will now
incorporate these constraints into an operational semantics that
defines a legal sequences of filter executions.

We represent a stream graph as a configuration $(\langle p(t_1),
n(t_1) \rangle,$ $\langle p(t_2), n(t_2) \rangle,$ $\dots,$ $\langle
p(t_k), n(t_k) \rangle)$, where $t_1 \dots t_k$ are the tapes in the
stream graph, and $p(t)$ represents the number of items that have been
popped from tape $t$.  Obviously, we have the constraint that $p(t)
\le n(t)$ for each tape $t$, since a filter can only pop as many items
as have appeared on its input tape.

When the program begins, no items have been pushed or popped from any
data channels.  Thus, each tape is empty, and the starting
configuration $C_0$ is simply the zero vector.  It is possible that
the initial configuration violates some of the constraints imposed by
the messaging and latency constructs, in which case the compiler can
inform the programmer that the delivery constraints requested in the
program are unsatisfiable.

Let ${\cal P}(C)$ denote whether or not the constraints in
Equations~\ref{eq:mc1} and~\ref{eq:mc2} are satisfied for all filters
in a stream graph with configuration $C$.  We can then write the
transition function between configurations as follows:
\[
\begin{scriptsize}
\begin{array}{c}
n(I_A) - p(I_A) \ge peek_A; \\ (\dots , \langle p(I_A), n(I_A) \rangle, \dots,
\langle p(O_A), n(O_A) \rangle, \dots); \\ {\cal P}((\dots , \langle p(I_A)+pop_A, n(I_A) \rangle, \dots,
\langle p(O_A), n(O_A)+push_A \rangle, \dots)); \\ \hline (\dots , \langle p(I_A)+pop_A,
n(I_A) \rangle, \dots, \langle p(O_A), n(O_A)+push_A \rangle, \dots)
\end{array}
\end{scriptsize}
\]
There are two components of this rule.  On the first line, we state
that, for filter $A$ to fire, there must be at least $peek_A$ items on
$A$'s input tape that have not yet been popped.  Secondly, we express
that once $A$ has fired, the new configuration must satisfy the
messaging and latency constraints ${\cal P}$.  The new configuration differs
from the original only in that $pop_A$ items have been popped from
$A$'s input and $push_A$ items have been pushed to $A$'s output.

\subsection{Denotational Semantics}

The operational semantics above defines the space of legal execution
orderings for the filters in a stream graph, but says nothing about
the values that are being pushed onto the tapes.  For this we develop
a denotational semantics, with which we can prove that certain
transformations preserve the meaning of the entire program.

Our denotational semantics contains three algebras: one for literal
StreamIt syntax, one for an intermediate abstract syntax, and one for
the semantic analysis.  The purpose of the intermediate algebra is to
provide a simplified syntax for developing stream transformations, and
to abstract away the StreamIt-specific aspects of the program.  We
provide an informal description of how to translate back and forth
between StreamIt programs and the abstract syntax, and then consider
more formal valuation functions for determining the meaning of the
abstract syntax within the semantic algebra.  Throughout the analysis,
we assume that filters are stateless and that the stream program is
semantically correct.

\subsubsection{Intermediate Algebra}
\label{sec:intalgebra}

The intermediate algebra provides a common mathematical representation
for manipulating stream programs.  Though we have referred to this
algebra as providing an abstract syntax for stream programs, the
representation is strictly a mathematical framework within semantic
domains rather than a program that is fit for execution.  Nonetheless,
the LISP-like syntax allows us to think of the representation as a
program that is amenable to straightforward transformation techniques.

\begin{figure}
\begin{align*}
&Item = Real \\
i~\in~&Index = N \\
g~\in~&IndexTransform = Index \ra Index \\
t~\in~&Tape = Index \ra Item \\
&Pop, Peek, Push = N \\ 
f~\in~&WorkStatement = IndexTransform \ra (Tape \ra Item) \\ 
&WorkFunction = WorkStatement^{+} \\
S~\in~&SplitType = \{Duplicate, WeightedRR\} \\ 
J~\in~&JoinType = \{WeightedRR\}
\end{align*}
\vspace{-18pt}
\caption{Semantic domains that are shared between the intermediate and
  transform algebras.
\protect\label{fig:shareddom}}
\vspace{-6pt}
\begin{align*}
s~\in~&Stream = Filter + Pipeline + SplitJoin + FeedbackLoop \\
&Filter = Push \times Pop \times Peek \times WorkFunction \\
&Pipeline = Stream^{+} \\
&SplitJoin = SplitType \times Stream^{+} \times JoinType \\
&InitFeedback = Int^{+} \\
&BodyStream, LoopStream = Stream \\
&FeedbackLoop = JoinType \times BodyStream \times \\
& \hspace{0.5in} LoopStream \times SplitType \times InitFeedback
\end{align*}
\vspace{-18pt}
\caption{Semantic domains specific to the intermediate algebra.
\protect\label{fig:interdom}}
\vspace{-6pt}
\begin{align*}
&StreamTransform =  IndexTransform \ra (Tape \ra Tape)
\end{align*}
\caption{Semantic domains specific to the transform algebra.
\protect\label{fig:transformdom}}
\end{figure}

The domains of the intermediate algebra are shown in
Figures~\ref{fig:shareddom}~and~\ref{fig:interdom}.  The algebra
represents tapes as an infinite mapping from an index to an item.
Generally, stream constructs are represented as a list of their
component streams, and filters' work functions are encoded as a list of
push statements that--given the transform from their local indexing to
the global tape position--returns a mapping from a tape to an output
item.

{\bf Converting to the intermediate algebra}.  It is straightforward
to generate an expression in the intermediate algebra that reflects
the meaning of a given StreamIt program.  Due to space limitations, we
consider here only the translation of the work functions.

The translation of a filter's work function contains two steps.  First,
the function is arranged in a {\it canonical form}, in which each pushed
item is given as a direct function of the peeked items, and all of the
pop statements are at the end of the function.  For example: \\
\vspace{0.2in}
\begin{scriptsize}
\begin{tabular}{l}
{\tt void work()}\{ \\
\hspace{12pt} {\tt output.push(} $f_1$ {\tt (input.peek(0), ... , input.peek(PEEK-1) )} \\
\hspace{12pt} $\dots$ \\
\hspace{12pt} {\tt output.push(} $f_{PUSH}$ {\tt (input.peek(0), ... , input.peek(PEEK-1) )} \\
\hspace{12pt} {\tt for (int i=0; i$<$POP; i++) \{ input.pop(); \}} \\
\}
\vspace{-12pt}
\end{tabular}
\end{scriptsize}
Above, we model the computation of the work function as pure
mathematical functions that can be injected into the semantic domain.
The algebraic work function, then, is the alternate application of each
push statement's function $f$, with the index expressions transformed
from their local index $x$ to a global index $g(x)$ on the input tape.
\begin{align*}
{\cal W}&[{\tt void~work() \{...\}}]: {\tt StreamIt Work} \ra
WorkFunction = \\
(&g \ra (t \ra f_1(t(g(0)), \dots , t(g(PEEK-1)))), \\
\dots \\
&g \ra (t \ra f_{PUSH}(t(g(0)), \dots , t(g(PEEK-1)))))
\end{align*}

{\bf Converting from the intermediate algebra}.  To convert back to
StreamIt, we can perform the inverse of the translation shown above,
with a push statement for each function and a local index expression $x$
in place of the global index $g(x)$.  Common sub-expression elimination
can be used to eliminate duplicate peek statements or shared portions of
the $f_i$'s.

\subsubsection{Transform Algebra}

The transform algebra is designed to express the meaning of a stream
graph as a transformation from an input tape to an output tape.  Its
semantic domains are given in Figures \ref{fig:interdom} and
\ref{fig:transformdom}.  We now give the valuation function ${\cal S}:
Stream \ra StreamTransform$ for converting from the intermediate
algebra to the transform algebra.  For a filter, we have:
\begin{align*}
&{\cal S} [(Filter~push~pop~peek~(f_1~\dots~f_{push}))] = g \ra (t \ra
(i \ra \\ &(f_{i~\%~push})(i_{local} \ra g(\mi{I_F}{O_F}(i) - peek + 1 + 
i_{local}))(t))
\end{align*}
That is, the value that a filter pushes onto the $i$'th position of its
output tape is calculated with its function at index $if~\%~push$.  By
the definition of $min$, the index offset to the last value the filter
peeks is $\mi{I_F}{O_F}(i)$, where $I_F$ and $O_F$ denote the input and
output tapes of the filter (as shown in Equation \ref{eq:minfilter},
this is a pure function of $push$, $pop$, and $peek$).  Thus, the offset
to the first value the filter peeks is $\mi{I_F}{O_F}(i) - peek + 1$,
and we obtain the global index by adding this offset to the local index
$i_{local}$.

For a pipeline, the transform function is simply the composition of the
transforms of component streams.  At the internal connections of the
pipeline, the index transform is the identity function, but at the start
of the pipeline we apply the transform $g$ to interface the pipeline to
its outside connection.
\begin{align*}
{\cal S} [(Pipeline~s_1~s_2~\dots~s_n)] = \\
g \ra ({\cal S}[s_n]({\cal I}) \circ \dots \circ {\cal S}[s_2]({\cal I}) \circ {\cal S}[s_1](g)) \\
where~{\cal I}~denotes~the~identity~function
\end{align*}
The valuation function for a SplitJoin follows the same idea, but the
notation is slightly heavier.  Given that we have a round robin joiner
with weights $w_1 \dots w_n$ and $W = \sum{w}$, we first represent the
parallel stream $p(i)$ which computes the $i$'th output of the
joiner:
\begin{align}
\label{eq:p}
p(i) = MIN(j~s.t.~\sum_{k=0}^{j-1}{w_i} \le i~mod~W)
\end{align}
Now, the $i$'th tape position assumes the value that is produced along
stream $p(i)$ in the SplitJoin, and the value of interest appears at
position $\mi{Ip(i)_J}{O_J}(i)$ on the output tape of stream $p(i)$.
The indexing function transforms the stream's local index $i_{local}$
for its own input tape to the corresponding index
$\mi{I_S}{Op(i)_S}(i_{local})$ for the input tape of the splitter:
\begin{align*}
&{\cal S} [(SplitJoin~S~s_1~s_2~\dots~s_n~J)] = g \ra (t \ra (i \ra \\
&(({\cal S}[s_{p(i)}])(i_{local} \ra
g(\mi{I_S}{Op(i)_S}(i_{local}))))(\mi{Ip(i)_J}{O_J}(i))))
\end{align*}
This completes the formulation of the transform algebra.  Using this
algebra, we can express the meaning of any StreamIt program as a
mathematical transformation between infinite tapes.  We will utilize
this formulation to prove that certain transformations of the stream
graph preserve the meaning of the program.

%% $\cal{S}$ 
%%   [(FeedbackLoop (. S) $s_body$ $s_loop$ (. J) ($init_1 \dots
%%   init_n$))] = ??????
