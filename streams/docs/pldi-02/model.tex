\begin{figure}
\centering
\psfig{figure=tapes.eps,width=3.2in}
\caption{A filter's input and output tapes during an execution step.
With each step, the filter pushes two items, pops two items, and peeks
at three additional items.  The initial state of the input tape is
shown at left.  The center shows the filter with both input and output
tapes during the invocation of {\tt work}.  The final state of the
output tape is shown at right.}
\label{fig:tape}
\end{figure}

\begin{figure}
\centering
\psfig{figure=pipeline.eps,width=2.0in}

(a) A Stream. \\
\vspace{8pt}
\psfig{figure=splitjoin.eps,width=3.2in}

(b) A SplitJoin. \\
\vspace{8pt}
\psfig{figure=feedback.eps,width=3.2in}

(c) A FeedbackLoop. \\
\vspace{8pt}
\caption{Tape labeling for StreamIt structures.}
\label{fig:tapelabels}
\end{figure}

\section{Streaming Model of Computation}

In this section, we develop an abstract model of streaming computation
to serve as a basis for reasoning about program transformations and
compilation techniques within the streaming domain.  A stream graph
differs from a traditional, sequential program in that all of the
filters of the graph are implicitly running in parallel, with the
execution order constrained only by the availability of data on
channels between the filters.  Further, filters communicate only with
their immediate neighbors, thereby removing any notion of global time
or non-local dependences of one filter on another.  [add idea that it
is the specification of the atomic work function that really prevents
global time] These properties merit the development of a new model of
computation, in which the notions of timing, scheduling, and
dependence analysis are in terms that are relative to a given filter
in the graph, instead of being global characteristics of a program.

In Section \ref{minfunc}, we develop a transfer function that provides
the basis for distributed time in a stream graph... [build operational
semantics to give a precise meaning to messaging, and denotational
semantics to validate program transformations].

\subsection{Notation}

We use the following notation:

\begin{itemize}

\item A {\it tape} is an infinite history of the values that have been
  pushed onto a channel between two filters (see Figure
  \ref{fig:tapes}).  We use $I_S$ and $O_S$ to denote the input and
  output tapes of stream $S$, respectively, with numbering used to
  distinguish between multiple input or output tapes (see Figure
  \ref{tapelabels}).  Finally, $n(T)$ represents the number of items
  on tape $T$ at a given point of execution.  [should we define $p(T)$
  here or wait until we use it?  long time def-use!]

\item We say that a filter $A$ is {\it upstream} of filter $B$ (or,
  equivalently, $B$ is {\it downstream} $A$) if there is a directed
  path in the stream graph from $O_A$ to $I_B$.  We use this
  terminology for tapes as well as filters.

\item The number of items that are pushed, popped, and peeked by
  filter $A$ during a single execution of its work function are
  denoted by $push_A$, $pop_A$, and $peek_A$, respectively.  Note that
  $peek_A$ includes the items that are popped, such that $pop_A \le
  peek_A$.

\end{itemize}

\subsection{Relative Time}
\label{sec:minfunc}

As outlined above, there is no concept of global time in a stream
graph since each filter is completely independent and can only
communicate with its neighbors through input and output channels.
Thus, if two filters need to synchronize an event, the synchronization
must be in terms of the data items that are passed over a channel.

In this context, we define a $min$ function between tapes in the
stream graph that allows disconnected filters to have a common notion
of time.  The function is defined in terms of data dependence:
\begin{definition}
$\mi{a}{b}(x)$ is the minimum number of items that must appear on tape
$a$ given that there are $x$ items on tape $b$.
\end{definition}

We now turn to deriving $\mi{a}{b}$ for all pairs of tapes $a$ and $b$
in a filter graph where $a$ is upstream of $b$.

\subsubsection{Filters}

Let us derive $\mi{I_A}{O_A}(x)$, which represents the time shift
across a single filter $A$.  Since the filter produces $push_A$ items
on every invocation, it must be invoked
$\left\lceil\frac{x}{push_A}\right\rceil$ to produce the $x$'th item.
On each invocation, it consumes $pop_A$ items, and peeks at an
additional $peek_A-pop_A$ items.  Thus, the total number of items that
must be present on the input is:
\begin{align*}
\mi{I_A}{O_A}(x) = \left\lceil\frac{x}{push_A}\right\rceil*pop_A+(peek_A-pop_A)
\end{align*}

\subsubsection{Pipelines}
\label{sec:timepipe}

Let us now derive an expression for $min$ in the case of a pipeline.
In the base case, consider that two filters are connected, with the
output of $A$ feeding into the input of $B$ (see
Figure~\ref{fig:tapelabels}).  We are seeking $\mi{I_A}{O_B}(x)$: the
minimum number of items that must appear on tape $I_A$ given that
there are $x$ items on tape $O_B$.  Observing that a minimum of
$\mi{I_B}{O_B}(x)$ items must appear on tape $I_B$, and that $I_B$
must equal $O_A$ since the filters are connected, we see that a
minimum of $(\mi{I_A}{O_A} \circ \ma{I_B}{O_B})(x)$ items must appear
on $I_A$:
\begin{align*}
\mi{I_A}{O_B} = \mi{I_A}{O_A} \circ \mi{I_B}{O_B}
\end{align*}
By identical reasoning, this composition law holds for pipelined
streams as well as filters.  That is, a Pipeline of streams $S1 \dots
Sn$ has the following $min$ function:
\begin{align}
\label{eq:composepipe}
\mi{S1}{Sn} &= \mi{I_{S1}}{O_{S1}} \circ \dots \circ \mi{I_{Sn}}{O_{Sn}}
\end{align}
One might be tempted to define the $min$ function for any pair of
connected tapes as the composition of functions for the operators
connecting those tapes.  However, such a definition turns out to be
problematic for the SplitJoin and FeedbackLoop constructs, which
require a slightly different composition law for their components (as
shown below).  Instead, we can further extend our notation to include
the {\it components} of streams that are connected in a pipeline.
That is, if tapes $t_i$ and $t_j$ are contained within stream
constructs $S_i$ and $S_j$, respectively, and $S_i$ and $S_j$ belong
to a pipeline of streams $S_1 \dots S_n$, then:
\begin{align}
\label{eq:composetape}
\mi{t_i}{t_j} &= \mi{t_i}{O_{S_i}} \circ \mi{I_{S_{i+1}}}{O_{S_{i+1}}}
\circ \dots \circ \mi{I_{S_j}}{t_j}
\end{align}

\subsubsection{SplitJoins}
\label{sec:timesj}

We now derive $min$ expressions for the components of a SplitJoin, and
for the SplitJoin construct as a whole.  We denote the $n$ output
tapes of the splitter $S$ by $O1_S \dots On_S$, and the $n$ input
tapes of the joiner $J$ by $I1_J \dots In_J$ (see Figure
\ref{tapelabels}).

{\bf Duplicate splitter.}  We consider the $i$'th output tape of an
$n$-way duplicating splitter.  Since the splitter duplicates each
input item onto each output tape, there must be at least $x$ items on
$I_S$ if there are $x$ items on $Oi_S$.  This yields a simple
expression for $min$:
\begin{align*}
\mi{I_S}{Oi_S}(x) = x
\end{align*}

{\bf Weighted round robin splitter.}  Let us consider an $n$-way
splitter with weights $w_1 \dots w_n$.  Observe that if there are
$n(On_S)$ items on the $n$'th output tape, then the splitter must have
executed $\floor{n(On_S)}{w_n}$ complete cycles in distributing items
to the output tapes; each cycle draws $sum_{i}{w_i}$ items from the
input tape $I_S$.  Further, if there are $n(Oi_S)$ items on the $i$'th
output tape, then $n(Oi_S) mod w_i$ additional items have been
deposited on $Oi_S$ during the current cycle of the splitter, and
$n(Oi_S)~mod~w_i + sum_{j=0}^{i-1}{w_j}$ items have been drawn from
the input since the last complete cycle.  Summing the item count for
the completed cycles and the current cycle gives the following
expression for $min$:
\begin{align*}
\mi{I_S}{Oi_S}(x) = \floor{n(On_S)}{w_n} * \sum_{i}{w_i} + x~mod~w_i +
\sum_{j=0}^{i-1}{w_j}
\end{align*}

{\bf Weighted round robin joiner.}  The reasoning is similar for an
$n$-way joiner with weights $w_1 \dots w_n$.  Let us use $W$ to denote
the sum of the weights: $W = \sum{i=1}^{n}{w_i}$.  If there are $x$
items on the output tape $O_J$, then the joiner must have executed
$\floor{x}{W}$ complete cycles, each of which drew $w_i$ items from
the $i$'th input tape.  Since the last complete cycle, the joiner has
drawn $x~mod~W$ items from its inputs, and $MIN(0, x~mod~W -
\sum_{j=0}^{i-1}{w_j})$ of these items were taken from input tape $j$.
Thus, the $min$ function from the output of the joiner to the $i$'th
input tape is as follows:
\begin{align*}
\mi{Ii_J}{O_J}(x) = w_i * W + MIN(0, x~mod~W - \sum_{j=0}^{i-1}{w_j})
\end{align*}

{\bf SplitJoin construct.}  As with the Pipeline construct, we can
derive the $min$ function across an entire SplitJoin as a composition
of the component functions.  However, a SplitJoin differs from a
Pipeline in that the joiner imposes a control dependence between the
parallel streams.  That is, for there to be $x$ items on the output of
the joiner, there must be at least $\mi{Ii_J}{O_J}(x)$ items on {\it
every} input tape $Ii_J$.  Applying the composition law for pipelines
(Equation \ref{eq:composepipe}), it follows that there must be at
least at least $\mi{Oi_S}{Ii_J} \circ \mi{Ii_J}{O_J}(x)$ items on
every output tape $Oi_S$ of the splitter.  Finally, the minimum number
of items appearing on the input tape $I_S$ of the splitter is the {\it
maximum} of the item requirement from any output tape $Oi_S$.  By this
reasoning, the $min$ function for a SplitJoin is as follows:
\begin{align*}
\mi{I_S}{O_J}(x) = MAX_{i \in [1,n]}~(\mi{I_S}{Oi_S} \circ
\mi{Oi_S}{Ii_J} \circ \mi{Ii_J}{O_J})(x)
\end{align*}

\subsubsection{FeedbackLoops}
\label{sec:timefl}

The $min$ function for a feedback loop requires extra care. Although
the feedback splitter $FS$ serves as a normal splitter, with the same
$min$ function as derived above, the feedback joiner $FJ$ is slightly
different due to the initialization phase of the loop.  Also, the
$min$ function does not compose across all components of the loop,
since otherwise there would be conflicting definitions for paths that
circle the loop several times.

{\bf Feedback joiner}.  For a feedback loop with delay $d$, the
feedback joiner must fabricate its first $d$ input values, since no
items have yet been pushed onto the loop tape $I2_{FJ}$.  This means
that there must be an offset of $d$ in the $min$ function, since the
first $d$ items are direct inputs to the joiner instead of appearing
as items on the input tape.  Using $J$ to denote a weighted round
robin joiner as considered above, we thus have the following
expression for the $min$ function across the feedback path:
\begin{align*}
\mi{I2_{FJ}}{O_{FJ}}(x) = \mi{I2_J}{O_J}(x) - d
\end{align*}
However, the $min$ function remains unchanged with respect to the
input from the main stream:
\begin{align*}
\mi{I1_{FJ}}{O_{FJ}}(x) = \mi{I1_J}{O_J}(x)
\end{align*}

{\bf Feedback components}.  Within a feedback loop, the $min$ function
between tape $a$ and any downstream tape $b$ can be uniquely defined
by composing the $min$ functions along the directed acyclic path
between $a$ and $b$.  We require an acyclic path to avoid successive
passes around the loop, which would prevent a unique definition of the
function.  Denoting this path of tapes by $(a, t_1, \dots , t_n, b)$,
the composition follows the form of Equation \ref{eq:composepipe}:
\begin{align*}
\mi{a}{b}(x) = \mi{a}{t_1} \circ \mi{t_1}{t_2} \circ \dots \mi{t_n}{b}
\end{align*}
Note that these functions can then be composed with those of
constructs neighboring the feedback loop to obtain, for instance, the
relation between the loop tape $I2_{FJ}$ and a downstream pipeline (by
application of Equation \ref{eq:composetape}).

{\bf Feedback loop construct.}  As a special case of the equation
above, we can see that the $min$ function for the feedback loop as a
whole is the composition of the $min$ functions along the main path:
\begin{align*}
\mi{I1_{FJ}}{O_{FS}}(x) = \mi{I1_J}{O_J}(x) \circ \mi{I1_J}{O_J}(x) 
\end{align*}
Intuitively, this is because--in any semantically correct stream
program--the loop itself is guaranteed to have enough inputs to feed
the joiner, such that the output tape of the feedback loop places a
restriction only on the input tape of the feedback loop.

\subsubsection{Summary}

In the preceding sections, we have derived a $min$ function for the
components of each stream construct, as well as for the stream
construct as a whole.  By application of Equation
\ref{eq:composetape}, this yields a function $\mi{a}{b}$ for every
pair of tapes $a$ and $b$ where $b$ is downstream of $a$.

[say some more profound stuff about the min function?  information
  flow, NOT data dependence.  The data dependence is in the work
  functions themselves; this is a property of the graphs.  distributed
  time.]

\subsection{Program Verification}

A number of program analysis techniques are immediately afforded by
the $min$ function.  In particular, it is very simple to compute 1)
whether or not the program will deadlock as a result of a starved
input channel, and 2) whether or not any buffer will grow without
bound during the steady-state execution of the program.

{\bf Deadlock detection.}  The deadlock detection algorithm takes
advantage of the fact that the only loops in our stream graph are part
of a FeedbackLoop construct.  A stream graph will be deadlock-free if
and only if every feedback loop produces enough data to satisfy its
own feedback joiner.  This can be formulated in terms of the $min$
function by considering $min_{t}{t}$, the data that a tape $t$ in a
feedback loop requires of itself.  However, since we didn't define
$min$ across circular paths in the stream graph, we will denote this
function by $minloop$ and define it at loop input to the feedback
joiner:
\begin{align*}
minloop(x) \equiv \mi{I2_{FJ}}{O_{FJ}} \circ \mi{O_{FJ}}{I2_{FJ}}
\end{align*}
Now, the loop will be deadlock-free if and only if $\forall x \in N, x
- minloop(x) > 0$.  This condition follows directly from
causality--the $x$'th item can be produced if and only if its
production depends only on some subset of the $x-1$ items that are
already on the channel.

{\bf Overflow detection.}  There are two places that a buffer can grow
to an unbounded size in the stream graph.  The first is in a feedback
loop, when\footnote{$f(x) = \omega(g(x))$ if $lim_{x \rightarrow
\infty}\frac{f(x)}{g(x)} = \infty$}~$x - minloop(x) = \omega(1)$.
That is, if $minloop(x)$ items on the feedback tape enables the
production of an additional $x - minloop(x)$ items that grows
asymptotically with the position $x$ on the tape, then the constant
consumption rate will not keep up with the growing production rate,
and the buffer will overflow.

The second case of buffer overflow is when the parallel streams of a
SplitJoin have asymptotically different production rates.  For a given
stream $i$ in a SplitJoin construct, the buffer corresponding to the
joiner input tape $Ii_J$ will overflow if and only if there is a
stream $j$ in the SplitJoin for which:
\begin{align*}
(\mi{I_S}{Oi_S}& \circ \mi{Oi_S}{Ii_J} \circ \mi{Ii_J}{O_J})(x) - \\
(\mi{I_S}{Oj_S}& \circ \mi{Oj_S}{Ij_J} \circ \mi{Ij_J}{O_J})(x) = \omega(1)
\end{align*}
Both of these cases could be detected by a compiler to verify that no
buffers will overflow during steady-state execution.

\subsection{Information Flow}

In the above sections, the $min$ function is mostly described in terms
of data dependences.  However, we can also think of this function as
defining a common timing mechanism that asynchronous filters can use
to synchronize events.  We present this timing mechanism in terms of
``information flow'', which we believe is a central concept of the
streaming domain.  Using this concept, we give a precise semantics to
message delivery timing in in StreamIt.

\subsubsection{Information Wavefronts}

When an item enters a stream, it carries with it some new information.
As execution progresses, this information cascades through the stream,
effecting the state of filters and the values of new data items which
are produced.  We refer to an ``information wavefront'' as the set of
filter executions that first sees the effects of a given input item.
Thus, although each filter's {\tt work} function is invoked
asynchronously without any notion of global time, two invocations of a
work function occur at the same ``information-relative time'' if they
operate on the same information wavefront.

The $min$ function can be used to give a precise definition to an
information wavefront.  One interpretation of $y = \mi{a}{b}(x)$ is
that the item at position $y$ of tape $a$ is the the latest item on
tape $a$ to {\it affect} the item at position $x$ of tape $b$.  This
is because item $x$ on tabe $b$ can be produced if and only if tape
$a$ contains at least $y$ items.  Note that this effect might be via a
control dependence rather than a data dependence--for instance, if
item $y$ needs to pass through a round-robin joiner before some data
from another stream can be routed to tape $b$.  This is why we choose
``information flow'' instead of ``data flow'' to describe the timing
concept.  Also, when speaking of the information wavefront, we only
consider information that is passed through the data streams; if a
data item affects another via a low-latency downstream message, then
this effect could jump ahead of the wavefront.

\subsubsection{Message Timing}

We now employ the $min$ function to give a precise meaning to the
message delivery guarantees in StreamIt.  Suppose that filter $A$
sends a message to filter $B$ with latency $\lambda$, where $\lambda$
is any integer.  After $\lambda$ invocations of $A$'s {\tt work}
function, $A$ will produce (or has produced) one or more data items
$d$.  Now, the messaging system guarantees that:
\begin{enumerate}

\item If $B$ is upstream of $A$, then $B$ will receive the message
immediately following the last invocation of its {\tt work} function
which produces items that affect $d$.

\item If $B$ is downstream of $A$, then $B$ will receive the message
immediately preceding the first invocation of its {\tt work} function
which produces items that are effected by $d$.

\end{enumerate}
Now let us express these constraints in terms of the $min$ function.
Letting $s$ be equal to $n(O_A)$ at the time that the message was
sent, we have that:
\begin{enumerate}

\item If $B$ is upstream of $A$, the message will be delivered when:
\begin{align}
\label{eq:msgup}
n(O_B) = push_B * \ceil{\mi{O_B}{O_A}(s + push_A * \lambda)}{push_B}
\end{align}
That is, $s + push_A * \lambda$ is the number of items on $A$'s output
tape after producing the data of interest.  Then, $y = \mi{O_B}{O_A}(s
+ push_A * \lambda)$ is the latest item on $B$'s output tape that
affects the data of interest.  The message should be delivered
immediately after the work function producing this item, which occurs
when the item count $n(O_B)$ equals $push_B * \ceil{y}{push_B}$, as
specified by the constraint.

\item If $B$ is downstream of $A$, the message will be delivered when:
\begin{align}
\label{eq:msgdown}
n(O_B) = MAX(x~s.t.~\mi{O_A}{O_B}(x) = s + push_A * (\lambda-1))
\end{align}
That is, $z = s + push_A * (\lambda - 1)$ is the number of items on
$A$'s output tape before pushing the data of interest.  The message
should be delivered when $B$ has executed as far as possible given
that there are $z$ items on $O_A$.  That is, $B$ should push $x$ items
on its output tape for the maximum $x$ that still depends on some of
the $z$ items, which is given by the condition $\mi{O_A}{O_B}(x) = z$
above.

Thus, when $A$ pushes the next set of data, it could affect the data
that will be pushed next onto the output tape of $B$.  (Note that the
next set of data from $A$ might not be sufficient to calculate the
next set on $B$'s output, but it could affect it nonetheless.)  The
message must be delivered immediately before this effected data
appears on $B$'s output, so the number of items $n(O_B)$ is as given
above.  We do not need a ceiling function as in (\label{eq:msgup})
because we are guaranteed to have the maximum $x$ at a multiple of
$push_B$.

\end{enumerate}

\subsubsection{Latency Constraints}

The same framework can be used to give a semantics to the latency
constraints in StreamIt.  Each directive MAX\_LATENCY(A, B, n) has the
same effect as defining a message from filter $B$ to upstream filter
$A$ with latency $n$.

\subsection{Operational Semantics}

\subsection{Denotational Semantics}

assume for now that the filters are stateless.

the above discussion addresses the order in which filters can be
executed, but does not address the values on the tapes, or the meaning
of a program as a whole.  for this we turn to a denotational
semantics, which will allow us to prove the equivalence of two
different stream programs.

in the denotaional semantics, we'll represent a filter as follows:

1. push, pop, (max items peeked)
2. the functions specifying the output values as a function of the
input values.

3. this set of functions...

ii. how to map a given textual representation into a canonical
representation.

iii. how to represent the canonical representation in the functinal
format

iv. further, to help automate program transformations, we can talk
about translationg from the functional format to the canonical
representation.  talk about CSE from reconstruction

in the next sections, we'll appeal to this denotational semantics in
argueing for the correctness of program transformations.

\section{Optimization}

then discuss a few optimizations with the denotational semantics to
prove that they are correct

