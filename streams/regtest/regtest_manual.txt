Documentation for StreamIT Regression Testing framework
Andrew Lamb (aalamb@mit.edu) 6/25/2002
$Id: regtest_manual.txt,v 1.1 2002-06-25 20:02:13 aalamb Exp $


Introduction

A new test (and regression test) framework exists in streams/regtest/ based on JUnit (www.junit.org), an open-source java based regression testing framework. The tests themselves are specified in java classes and typically refer to streamit program sources and sample output. The Makefile in the streams/regtest controls the regression testing.

Each streamit program in the regression testing framework can be automatically compiled and run, with its output checked against the contents of a file you specifty. Both the uniprocessor backend and the RAW backend are supported. You can also check simply that a program compiles but not check its output.



Running a Regression Test

The easiest way to run the regression test suite is to go to the regtest directory and run make. However, this will take a long time (like overnight long) beccause each test program is compiled using several different optimizations . A far better way to use the regression testing framework on a regular basis is to add a regression test for the specific program you are working on.

Based on the makefile, either a text user interface or a graphical user interface can be used by JUnit. The graphical UI reports failures in detail as they occur. The text UI waits until all tests are run before issuing its detailed report. 

As the regression tests are run, all error messages are placed in the file regtest_errors.txt. In general, a test fails only when some external utility returns with a non-zero error message. However, even if 0 is returned, any output to stderr is dumped to regtest_errors.txt. If a non-zero result is returned, then stdout is also dumped to regtest_errors.txt. After all of the regression tests have been run, regtest_errors.txt provides a useful way to perform a post-mortem on any test that fails.


Adding New Tests (Do Often)

The easiest way to add another program regression test is to modify the java files in the streams/regtest/streamittest directory. Specifically, TestExamples.java and TestApps.java contain code for executing and testing examples in the streams/docs/examples/hand/ directoy and apps in the streams/Apps. To add a test, you need to do two things:
1. Make a public void method named something like newTest()
2. Add a line (like the ones already present) in the suite() method with something like 
   suite.addTest(new TestExamples("testFieldProp2", flags));

Sometimes you might want to make your own suite (say if you want to be able to run some tests individually, but still have them integrated into the regression testing framework). The procedure to add a new TestSuite is straightforward:
1. Copy streamittest/TestTemplate.java into your new test file.
2. Replace TestTemplate with your new test name
3. Add a line in TestAll .makeTestSuite like
   suite.addTest(TestTemplate.suite(flags));

And you are all set to go. If you want to run only your new tests, run either of the commands:
(graphical UI) junit.swingui.TestRunner streamittest.TestApps
(textui UI)    junit.textui.TestRunner  streamittest.TestApps

replacing streamittest.TestApps with your class's completely qualified name.


Internals Explanation and Organization

All tests in JUnit are derived from the TestCase class. TestCase is a bit of a misnomer because a single TestCase class often contains numerous individual test cases. TestCases can be run directly by the a TestRunner, or they can be grouped together into TestSuites. (You can execute a TestRunners on an individual TestCase as shown above).

All TestCases for streamit extend StreamITTestCase, which contains useful utility procedures for executing and verifying a program's output. 

The basic idea of Harness, CompilerHarness and RuntimeHarness is to abstract away (via several levels of indirection) the nonsense involved with executing native commands in java. CompilerInterface is provided to abstract away the calls to the compiler and to encapsulate the compiler options. This insulates StreamItTestCase, and we don't need to pass around compiler options all over the place. ResultPrinter is the central control station via which all output is generated. By outputting via ResultPrinter, capturing relevant output becomes easy.

Success Definitions (checked for in streamittest.Harness):
Streamit Compilation: java returns 0
Gcc Compilation: gcc returns 0
Execution: app returns 0
Compare: cmp returns 0
Raw Compilation: auto generated make returns 0
Raw Execution:   auto generated "make run" return 0



